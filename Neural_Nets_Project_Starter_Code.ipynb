{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gesture Recognition\n",
    "In this group project, you are going to build a 3D Conv model that will be able to predict the 5 gestures correctly. Please import the following libraries to get started."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from scipy.misc import imread, imresize\n",
    "import datetime\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We set the random seed so that the results don't vary drastically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(30)\n",
    "import random as rn\n",
    "rn.seed(30)\n",
    "from keras import backend as K\n",
    "import tensorflow as tf\n",
    "tf.set_random_seed(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this block, you read the folder names for training and validation. You also set the `batch_size` here. Note that you set the batch size in such a way that you are able to use the GPU in full capacity. You keep increasing the batch size until the machine throws an error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_doc = np.random.permutation(open('./Project_data/train.csv').readlines())\n",
    "val_doc = np.random.permutation(open('./Project_data/val.csv').readlines())\n",
    "batch_size = 10 #experiment with the batch size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "663\n"
     ]
    }
   ],
   "source": [
    "print(len(train_doc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n"
     ]
    }
   ],
   "source": [
    "print(len(val_doc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper Functions and our current preprogessing pipeline\n",
    "# pre process ===> conbert_to_grayscale --> normalize_data\n",
    "\n",
    "# Convert to grayscale\n",
    "def convert_to_grayscale(data):\n",
    "    return data.mean(axis=-1,keepdims=1) \n",
    "\n",
    "def normalize_data(data):\n",
    "    return data/127.5-1\n",
    "\n",
    "def preprocess_pipeline(data):\n",
    "    return normalize_data(convert_to_grayscale(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generator\n",
    "This is one of the most important part of the code. The overall structure of the generator has been given. In the generator, you are going to preprocess the images as you have images of 2 different dimensions as well as create a batch of video frames. You have to experiment with `img_idx`, `y`,`z` and normalization such that you get high accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator(source_path, folder_list, batch_size):\n",
    "    \n",
    "    x = 30 # number of frames\n",
    "    y = 120 # image width\n",
    "    z = 120 # image height\n",
    "    \n",
    "    print( 'Source path = ', source_path, '; batch size =', batch_size)\n",
    "    img_idx = [x for x in range(0,x,2)] #create a list of image numbers you want to use for a particular video\n",
    "    while True:\n",
    "        t = np.random.permutation(folder_list)\n",
    "        num_batches = len(folder_list)//batch_size # calculate the number of batches\n",
    "        for batch in range(num_batches): # we iterate over the number of batches\n",
    "            batch_data = np.zeros((batch_size,x,y,z,3)) # x is the number of images you use for each video, (y,z) is the final size of the input images and 3 is the number of channels RGB\n",
    "            batch_labels = np.zeros((batch_size,5)) # batch_labels is the one hot representation of the output\n",
    "            for folder in range(batch_size): # iterate over the batch_size\n",
    "                imgs = os.listdir(source_path+'/'+ t[folder + (batch*batch_size)].split(';')[0]) # read all the images in the folder\n",
    "                for idx,item in enumerate(img_idx): #  Iterate iver the frames/images of a folder to read them in\n",
    "                    image = imread(source_path+'/'+ t[folder + (batch*batch_size)].strip().split(';')[0]+'/'+imgs[item]).astype(np.float32)\n",
    "                    \n",
    "                    #crop the images and resize them. Note that the images are of 2 different shape \n",
    "                    #and the conv3D will throw error if the inputs in a batch have different shapes\n",
    "                    \n",
    "                    temp = imresize(image,(120,120))\n",
    "                    temp = normalize_data(temp)\n",
    "                    \n",
    "                    batch_data[folder,idx,:,:,0] = (temp[:,:,0])\n",
    "                    batch_data[folder,idx,:,:,1] = (temp[:,:,1])\n",
    "                    batch_data[folder,idx,:,:,2] = (temp[:,:,2])\n",
    "                    \n",
    "                batch_labels[folder, int(t[folder + (batch*batch_size)].strip().split(';')[2])] = 1\n",
    "            yield batch_data, batch_labels #you yield the batch_data and the batch_labels, remember what does yield do\n",
    "\n",
    "        \n",
    "        # write the code for the remaining data points which are left after full batches\n",
    "        \n",
    "        if (len(folder_list) != batch_size*num_batches):\n",
    "            print(\"Batch: \",num_batches+1,\"Index:\", batch_size)\n",
    "            batch_size = len(folder_list) - (batch_size*num_batches)\n",
    "            batch_data = np.zeros((batch_size,x,y,z,c)) # x is the number of images you use for each video, (y,z) is the final size of the input images and 3 is the number of channels RGB\n",
    "            batch_labels = np.zeros((batch_size,5)) # batch_labels is the one hot representation of the output\n",
    "            for folder in range(batch_size): # iterate over the batch_size\n",
    "                imgs = os.listdir(source_path+'/'+ t[folder + (batch*batch_size)].split(';')[0]) # read all the images in the folder\n",
    "                for idx,item in enumerate(img_idx): #  Iterate iver the frames/images of a folder to read them in\n",
    "                    image = imread(source_path+'/'+ t[folder + (batch*batch_size)].strip().split(';')[0]+'/'+imgs[item]).astype(np.float32)\n",
    "                    \n",
    "                    #crop the images and resize them. Note that the images are of 2 different shape \n",
    "                    #and the conv3D will throw error if the inputs in a batch have different shapes\n",
    "                    temp = imresize(image,(120,120))\n",
    "                    temp = normalize_data(temp)\n",
    "                    \n",
    "                    batch_data[folder,idx,:,:,0] = (temp[:,:,0])\n",
    "                    batch_data[folder,idx,:,:,1] = (temp[:,:,1])\n",
    "                    batch_data[folder,idx,:,:,2] = (temp[:,:,2])\n",
    "                   \n",
    "                batch_labels[folder, int(t[folder + (batch*batch_size)].strip().split(';')[2])] = 1\n",
    "            yield batch_data, batch_labels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note here that a video is represented above in the generator as (number of images, height, width, number of channels). Take this into consideration while creating the model architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# training sequences = 663\n",
      "# validation sequences = 100\n",
      "# epochs = 10\n"
     ]
    }
   ],
   "source": [
    "curr_dt_time = datetime.datetime.now()\n",
    "train_path = './Project_data/train'\n",
    "val_path = './Project_data/val'\n",
    "num_train_sequences = len(train_doc)\n",
    "print('# training sequences =', num_train_sequences)\n",
    "num_val_sequences = len(val_doc)\n",
    "print('# validation sequences =', num_val_sequences)\n",
    "num_epochs = 10# choose the number of epochs\n",
    "print ('# epochs =', num_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "Here you make the model using different functionalities that Keras provides. Remember to use `Conv3D` and `MaxPooling3D` and not `Conv2D` and `Maxpooling2D` for a 3D convolution model. You would want to use `TimeDistributed` while building a Conv2D + RNN model. Also remember that the last layer is the softmax. Design the network in such a way that the model is able to give good accuracy on the least number of parameters so that it can fit in the memory of the webcam."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = 3\n",
    "nb_filters = [8,16,32,64]\n",
    "nb_dense = [1000, 500, 5]\n",
    "input_shape = (30, 120, 120, c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, GRU, Flatten, TimeDistributed, Flatten, BatchNormalization, Activation, Dropout\n",
    "from keras.layers.convolutional import Conv3D, MaxPooling3D\n",
    "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n",
    "from keras import optimizers\n",
    "\n",
    "#write your model here\n",
    "\n",
    "model_a = Sequential()\n",
    "\n",
    "model_a.add(Conv3D(nb_filters[0], \n",
    "                 kernel_size=(3,3,3), \n",
    "                 input_shape=input_shape,\n",
    "                 padding='same'))\n",
    "model_a.add(BatchNormalization())\n",
    "model_a.add(Activation('relu'))\n",
    "\n",
    "model_a.add(MaxPooling3D(pool_size=(2,2,2)))\n",
    "\n",
    "model_a.add(Conv3D(nb_filters[1], \n",
    "                 kernel_size=(3,3,3), \n",
    "                 padding='same'))\n",
    "model_a.add(BatchNormalization())\n",
    "model_a.add(Activation('relu'))\n",
    "\n",
    "model_a.add(MaxPooling3D(pool_size=(2,2,2)))\n",
    "\n",
    "model_a.add(Conv3D(nb_filters[2], \n",
    "                 kernel_size=(1,3,3), \n",
    "                 padding='same'))\n",
    "model_a.add(BatchNormalization())\n",
    "model_a.add(Activation('relu'))\n",
    "\n",
    "model_a.add(MaxPooling3D(pool_size=(2,2,2)))\n",
    "\n",
    "model_a.add(Conv3D(nb_filters[3], \n",
    "                 kernel_size=(1,3,3), \n",
    "                 padding='same'))\n",
    "model_a.add(BatchNormalization())\n",
    "model_a.add(Activation('relu'))\n",
    "\n",
    "model_a.add(MaxPooling3D(pool_size=(2,2,2)))\n",
    "\n",
    "#Flatten Layers\n",
    "model_a.add(Flatten())\n",
    "\n",
    "model_a.add(Dense(nb_dense[0], activation='relu'))\n",
    "model_a.add(Dropout(0.5))\n",
    "\n",
    "model_a.add(Dense(nb_dense[1], activation='relu'))\n",
    "model_a.add(Dropout(0.5))\n",
    "\n",
    "#softmax layer\n",
    "model_a.add(Dense(nb_dense[2], activation='softmax'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you have written the model, the next step is to `compile` the model. When you print the `summary` of the model, you'll see the total number of parameters you have to train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv3d_1 (Conv3D)            (None, 30, 120, 120, 8)   656       \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 30, 120, 120, 8)   32        \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 30, 120, 120, 8)   0         \n",
      "_________________________________________________________________\n",
      "max_pooling3d_1 (MaxPooling3 (None, 15, 60, 60, 8)     0         \n",
      "_________________________________________________________________\n",
      "conv3d_2 (Conv3D)            (None, 15, 60, 60, 16)    3472      \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 15, 60, 60, 16)    64        \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 15, 60, 60, 16)    0         \n",
      "_________________________________________________________________\n",
      "max_pooling3d_2 (MaxPooling3 (None, 7, 30, 30, 16)     0         \n",
      "_________________________________________________________________\n",
      "conv3d_3 (Conv3D)            (None, 7, 30, 30, 32)     4640      \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 7, 30, 30, 32)     128       \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 7, 30, 30, 32)     0         \n",
      "_________________________________________________________________\n",
      "max_pooling3d_3 (MaxPooling3 (None, 3, 15, 15, 32)     0         \n",
      "_________________________________________________________________\n",
      "conv3d_4 (Conv3D)            (None, 3, 15, 15, 64)     18496     \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 3, 15, 15, 64)     256       \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 3, 15, 15, 64)     0         \n",
      "_________________________________________________________________\n",
      "max_pooling3d_4 (MaxPooling3 (None, 1, 7, 7, 64)       0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 3136)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1000)              3137000   \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 1000)              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 500)               500500    \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 500)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 5)                 2505      \n",
      "=================================================================\n",
      "Total params: 3,667,749\n",
      "Trainable params: 3,667,509\n",
      "Non-trainable params: 240\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "rate = 0.001\n",
    "optimiser = optimizers.Adam(lr=rate) #write your optimizer\n",
    "model_a.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "print (model_a.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us create the `train_generator` and the `val_generator` which will be used in `.fit_generator`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator = generator(train_path, train_doc, batch_size)\n",
    "val_generator = generator(val_path, val_doc, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'model_init' + '_' + str(curr_dt_time).replace(' ','').replace(':','_') + '/'\n",
    "    \n",
    "if not os.path.exists(model_name):\n",
    "    os.mkdir(model_name)\n",
    "        \n",
    "filepath = model_name + 'model-{epoch:05d}-{loss:.5f}-{categorical_accuracy:.5f}-{val_loss:.5f}-{val_categorical_accuracy:.5f}.h5'\n",
    "\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=False, save_weights_only=False, mode='auto', period=1)\n",
    "\n",
    "LR = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=2, cooldown=1, verbose=1)# write the Reducelronplateau code here\n",
    "callbacks_list = [checkpoint, LR]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `steps_per_epoch` and `validation_steps` are used by `fit_generator` to decide the number of next() calls it need to make."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if (num_train_sequences%batch_size) == 0:\n",
    "    steps_per_epoch = int(num_train_sequences/batch_size)\n",
    "else:\n",
    "    steps_per_epoch = (num_train_sequences//batch_size) + 1\n",
    "\n",
    "if (num_val_sequences%batch_size) == 0:\n",
    "    validation_steps = int(num_val_sequences/batch_size)\n",
    "else:\n",
    "    validation_steps = (num_val_sequences//batch_size) + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now fit the model. This will start training the model and with the help of the checkpoints, you'll be able to save the model at the end of each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source path =  ./Project_data/val ; batch size = 10\n",
      "Source path =  ./Project_data/train ; batch size = 10\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/disks/user/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:18: DeprecationWarning: `imread` is deprecated!\n",
      "`imread` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``imageio.imread`` instead.\n",
      "/mnt/disks/user/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:23: DeprecationWarning: `imresize` is deprecated!\n",
      "`imresize` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``skimage.transform.resize`` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65/67 [============================>.] - ETA: 3s - loss: 5.7957 - categorical_accuracy: 0.2662Batch:  67 Index: 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/disks/user/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:44: DeprecationWarning: `imread` is deprecated!\n",
      "`imread` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``imageio.imread`` instead.\n",
      "/mnt/disks/user/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:48: DeprecationWarning: `imresize` is deprecated!\n",
      "`imresize` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``skimage.transform.resize`` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "67/67 [==============================] - 127s 2s/step - loss: 5.7062 - categorical_accuracy: 0.2647 - val_loss: 1.4927 - val_categorical_accuracy: 0.4000\n",
      "\n",
      "Epoch 00001: saving model to model_init_2020-12-2104_40_49.443651/model-00001-5.74708-0.26395-1.49271-0.40000.h5\n",
      "Epoch 2/10\n",
      "67/67 [==============================] - 14s 208ms/step - loss: 1.9267 - categorical_accuracy: 0.3284 - val_loss: 1.7683 - val_categorical_accuracy: 0.3700\n",
      "\n",
      "Epoch 00002: saving model to model_init_2020-12-2104_40_49.443651/model-00002-1.92671-0.32836-1.76834-0.37000.h5\n",
      "Epoch 3/10\n",
      "67/67 [==============================] - 15s 217ms/step - loss: 1.5844 - categorical_accuracy: 0.3184 - val_loss: 1.2554 - val_categorical_accuracy: 0.4700\n",
      "\n",
      "Epoch 00003: saving model to model_init_2020-12-2104_40_49.443651/model-00003-1.58445-0.31841-1.25539-0.47000.h5\n",
      "Epoch 4/10\n",
      "67/67 [==============================] - 14s 209ms/step - loss: 1.4098 - categorical_accuracy: 0.3881 - val_loss: 1.1900 - val_categorical_accuracy: 0.5000\n",
      "\n",
      "Epoch 00004: saving model to model_init_2020-12-2104_40_49.443651/model-00004-1.40981-0.38806-1.19001-0.50000.h5\n",
      "Epoch 5/10\n",
      "67/67 [==============================] - 15s 225ms/step - loss: 1.3183 - categorical_accuracy: 0.4229 - val_loss: 1.1988 - val_categorical_accuracy: 0.4300\n",
      "\n",
      "Epoch 00005: saving model to model_init_2020-12-2104_40_49.443651/model-00005-1.31825-0.42289-1.19878-0.43000.h5\n",
      "Epoch 6/10\n",
      "67/67 [==============================] - 15s 222ms/step - loss: 1.4143 - categorical_accuracy: 0.3682 - val_loss: 1.0617 - val_categorical_accuracy: 0.5300\n",
      "\n",
      "Epoch 00006: saving model to model_init_2020-12-2104_40_49.443651/model-00006-1.41430-0.36816-1.06166-0.53000.h5\n",
      "Epoch 7/10\n",
      "67/67 [==============================] - 15s 217ms/step - loss: 1.4100 - categorical_accuracy: 0.3682 - val_loss: 1.1794 - val_categorical_accuracy: 0.5200\n",
      "\n",
      "Epoch 00007: saving model to model_init_2020-12-2104_40_49.443651/model-00007-1.40999-0.36816-1.17935-0.52000.h5\n",
      "Epoch 8/10\n",
      "67/67 [==============================] - 14s 211ms/step - loss: 1.2104 - categorical_accuracy: 0.4279 - val_loss: 1.0351 - val_categorical_accuracy: 0.5700\n",
      "\n",
      "Epoch 00008: saving model to model_init_2020-12-2104_40_49.443651/model-00008-1.21037-0.42786-1.03511-0.57000.h5\n",
      "Epoch 9/10\n",
      "67/67 [==============================] - 15s 219ms/step - loss: 1.0799 - categorical_accuracy: 0.5572 - val_loss: 1.3684 - val_categorical_accuracy: 0.4300\n",
      "\n",
      "Epoch 00009: saving model to model_init_2020-12-2104_40_49.443651/model-00009-1.07992-0.55721-1.36838-0.43000.h5\n",
      "Epoch 10/10\n",
      "67/67 [==============================] - 14s 206ms/step - loss: 1.1841 - categorical_accuracy: 0.5075 - val_loss: 1.1701 - val_categorical_accuracy: 0.4700\n",
      "\n",
      "Epoch 00010: saving model to model_init_2020-12-2104_40_49.443651/model-00010-1.18411-0.50746-1.17010-0.47000.h5\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fe1cc8f4dd8>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_a.fit_generator(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1, \n",
    "                    callbacks=callbacks_list, validation_data=val_generator, \n",
    "                    validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The validation accuracy is 57%. Let us try some more models with different combination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "nb_frames = 30 # number of frames\n",
    "nb_rows = 120 # image width\n",
    "nb_cols = 120 # image height \n",
    "\n",
    "nb_classes = 5\n",
    "nb_channel = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### New Generator\n",
    "- Let us try a new generator function with the preprocess pipeline function that we have defined above to convert the images to greyscale and then normalize it and then feed. Let us try to improve the performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_generator(source_path, folder_list, batch_size):\n",
    "      \n",
    "    print( 'Source path = ', source_path, '; batch size =', batch_size)\n",
    "    img_idx = [x for x in range(0,nb_frames)] #create a list of image numbers you want to use for a particular video\n",
    "    while True:\n",
    "        t = np.random.permutation(folder_list)\n",
    "        num_batches = len(folder_list)//batch_size # calculate the number of batches\n",
    "        for batch in range(num_batches): # we iterate over the number of batches\n",
    "            batch_data = np.zeros((batch_size,nb_frames,nb_rows,nb_cols,nb_channel)) # x is the number of images you use for each video, (y,z) is the final size of the input images and 3 is the number of channels RGB\n",
    "            batch_labels = np.zeros((batch_size,5)) # batch_labels is the one hot representation of the output\n",
    "            for folder in range(batch_size): # iterate over the batch_size\n",
    "                imgs = os.listdir(source_path+'/'+ t[folder + (batch*batch_size)].split(';')[0]) # read all the images in the folder\n",
    "                for idx,item in enumerate(img_idx): #  Iterate iver the frames/images of a folder to read them in\n",
    "                    image = imread(source_path+'/'+ t[folder + (batch*batch_size)].strip().split(';')[0]+'/'+imgs[item]).astype(np.float32)\n",
    "                    \n",
    "                    #crop the images and resize them. Note that the images are of 2 different shape \n",
    "                    #and the conv3D will throw error if the inputs in a batch have different shapes\n",
    "                    temp = imresize(image,(nb_rows,nb_cols))\n",
    "                    #temp = normalize_data(temp)\n",
    "                    temp = preprocess_pipeline(temp)\n",
    "                    \n",
    "                    batch_data[folder,idx] = temp\n",
    "\n",
    "       \n",
    "                batch_labels[folder, int(t[folder + (batch*batch_size)].strip().split(';')[2])] = 1\n",
    "            yield batch_data, batch_labels #you yield the batch_data and the batch_labels, remember what does yield do\n",
    "\n",
    "        \n",
    "        # write the code for the remaining data points which are left after full batches\n",
    "        if (len(folder_list) != batch_size*num_batches):\n",
    "            print(\"Batch: \",num_batches+1,\"Index:\", batch_size)\n",
    "            batch_size = len(folder_list) - (batch_size*num_batches)\n",
    "            batch_data = np.zeros((batch_size,nb_frames,nb_rows,nb_cols,nb_channel)) # x is the number of images you use for each video, (y,z) is the final size of the input images and 3 is the number of channels RGB\n",
    "            batch_labels = np.zeros((batch_size,5)) # batch_labels is the one hot representation of the output\n",
    "            for folder in range(batch_size): # iterate over the batch_size\n",
    "                imgs = os.listdir(source_path+'/'+ t[folder + (batch*batch_size)].split(';')[0]) # read all the images in the folder\n",
    "                for idx,item in enumerate(img_idx): #  Iterate iver the frames/images of a folder to read them in\n",
    "                    image = imread(source_path+'/'+ t[folder + (batch*batch_size)].strip().split(';')[0]+'/'+imgs[item]).astype(np.float32)\n",
    "                    \n",
    "                    #crop the images and resize them. Note that the images are of 2 different shape \n",
    "                    #and the conv3D will throw error if the inputs in a batch have different shapes\n",
    "                    temp = imresize(image,(nb_rows,nb_cols))\n",
    "                    #temp = normalize_data(temp)\n",
    "                    temp = preprocess_pipeline(temp)\n",
    "                    \n",
    "                    batch_data[folder,idx] = temp\n",
    "                   \n",
    "                batch_labels[folder, int(t[folder + (batch*batch_size)].strip().split(';')[2])] = 1\n",
    "            yield batch_data, batch_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Importing Adam optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.losses import categorical_crossentropy\n",
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 2:\n",
    "- Using 2 Conv3D layers with filter as 32 then Max Pooling to extract features and dropout of 0.25 to avoid overfitting\n",
    "- Using 2 Conv3D layers with filter as 64 then Max Pooling to extract features and dropout of 0.25 to avoid overfitting\n",
    "- Kernel size is (3,3,3)\n",
    "- Feeding it to MLP architecture. One Dense layer with 'Relu' and final with Softmax activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv3d_13 (Conv3D)           (None, 30, 120, 120, 32)  2624      \n",
      "_________________________________________________________________\n",
      "activation_13 (Activation)   (None, 30, 120, 120, 32)  0         \n",
      "_________________________________________________________________\n",
      "conv3d_14 (Conv3D)           (None, 30, 120, 120, 32)  27680     \n",
      "_________________________________________________________________\n",
      "activation_14 (Activation)   (None, 30, 120, 120, 32)  0         \n",
      "_________________________________________________________________\n",
      "max_pooling3d_13 (MaxPooling (None, 10, 40, 40, 32)    0         \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 10, 40, 40, 32)    0         \n",
      "_________________________________________________________________\n",
      "conv3d_15 (Conv3D)           (None, 10, 40, 40, 64)    55360     \n",
      "_________________________________________________________________\n",
      "activation_15 (Activation)   (None, 10, 40, 40, 64)    0         \n",
      "_________________________________________________________________\n",
      "conv3d_16 (Conv3D)           (None, 10, 40, 40, 64)    110656    \n",
      "_________________________________________________________________\n",
      "activation_16 (Activation)   (None, 10, 40, 40, 64)    0         \n",
      "_________________________________________________________________\n",
      "max_pooling3d_14 (MaxPooling (None, 4, 14, 14, 64)     0         \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 4, 14, 14, 64)     0         \n",
      "_________________________________________________________________\n",
      "flatten_4 (Flatten)          (None, 50176)             0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 512)               25690624  \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 5)                 2565      \n",
      "=================================================================\n",
      "Total params: 25,889,509\n",
      "Trainable params: 25,889,509\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Define model\n",
    "model = Sequential()\n",
    "model.add(Conv3D(32, kernel_size=(3, 3, 3), input_shape=(nb_frames,nb_rows,nb_cols,nb_channel), padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv3D(32, kernel_size=(3, 3, 3), padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling3D(pool_size=(3, 3, 3), padding='same'))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Conv3D(64, kernel_size=(3, 3, 3), padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv3D(64, kernel_size=(3, 3, 3), padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling3D(pool_size=(3, 3, 3), padding='same'))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(nb_classes, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer=Adam(), loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- train_generator and val_generator to be used in .fit_generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator = new_generator(train_path, train_doc, batch_size)\n",
    "val_generator = new_generator(val_path, val_doc, batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Fitting the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source path =  ./Project_data/val ; batch size = 10\n",
      "Source path =  Epoch 1/10\n",
      "./Project_data/train ; batch size = 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/disks/user/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:14: DeprecationWarning: `imread` is deprecated!\n",
      "`imread` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``imageio.imread`` instead.\n",
      "  \n",
      "/mnt/disks/user/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:18: DeprecationWarning: `imresize` is deprecated!\n",
      "`imresize` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``skimage.transform.resize`` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64/67 [===========================>..] - ETA: 6s - loss: 1.7857 - categorical_accuracy: 0.2031Batch:  67 Index: 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/disks/user/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:38: DeprecationWarning: `imread` is deprecated!\n",
      "`imread` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``imageio.imread`` instead.\n",
      "/mnt/disks/user/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:42: DeprecationWarning: `imresize` is deprecated!\n",
      "`imresize` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``skimage.transform.resize`` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "67/67 [==============================] - 162s 2s/step - loss: 1.7778 - categorical_accuracy: 0.1956 - val_loss: 1.6096 - val_categorical_accuracy: 0.2100\n",
      "\n",
      "Epoch 00001: saving model to model_init_2020-12-2013_19_00.364355/model-00001-1.77960-0.19759-1.60958-0.21000.h5\n",
      "Epoch 2/10\n",
      "67/67 [==============================] - 46s 681ms/step - loss: 1.6093 - categorical_accuracy: 0.2189 - val_loss: 1.6089 - val_categorical_accuracy: 0.2100\n",
      "\n",
      "Epoch 00002: saving model to model_init_2020-12-2013_19_00.364355/model-00002-1.60934-0.21891-1.60889-0.21000.h5\n",
      "Epoch 3/10\n",
      "67/67 [==============================] - 46s 681ms/step - loss: 1.6131 - categorical_accuracy: 0.1393 - val_loss: 1.6081 - val_categorical_accuracy: 0.2100\n",
      "\n",
      "Epoch 00003: saving model to model_init_2020-12-2013_19_00.364355/model-00003-1.61311-0.13930-1.60806-0.21000.h5\n",
      "Epoch 4/10\n",
      "67/67 [==============================] - 46s 680ms/step - loss: 1.6101 - categorical_accuracy: 0.2040 - val_loss: 1.6085 - val_categorical_accuracy: 0.2100\n",
      "\n",
      "Epoch 00004: saving model to model_init_2020-12-2013_19_00.364355/model-00004-1.61010-0.20398-1.60846-0.21000.h5\n",
      "Epoch 5/10\n",
      "67/67 [==============================] - 46s 681ms/step - loss: 1.6069 - categorical_accuracy: 0.2537 - val_loss: 1.6075 - val_categorical_accuracy: 0.2100\n",
      "\n",
      "Epoch 00005: saving model to model_init_2020-12-2013_19_00.364355/model-00005-1.60686-0.25373-1.60745-0.21000.h5\n",
      "Epoch 6/10\n",
      "67/67 [==============================] - 46s 680ms/step - loss: 1.6106 - categorical_accuracy: 0.1841 - val_loss: 1.6075 - val_categorical_accuracy: 0.2100\n",
      "\n",
      "Epoch 00006: saving model to model_init_2020-12-2013_19_00.364355/model-00006-1.61061-0.18408-1.60749-0.21000.h5\n",
      "Epoch 7/10\n",
      "67/67 [==============================] - 46s 680ms/step - loss: 1.6097 - categorical_accuracy: 0.1990 - val_loss: 1.6078 - val_categorical_accuracy: 0.2100\n",
      "\n",
      "Epoch 00007: saving model to model_init_2020-12-2013_19_00.364355/model-00007-1.60969-0.19900-1.60777-0.21000.h5\n",
      "\n",
      "Epoch 00007: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "Epoch 8/10\n",
      "67/67 [==============================] - 46s 680ms/step - loss: 1.6094 - categorical_accuracy: 0.1940 - val_loss: 1.6079 - val_categorical_accuracy: 0.2100\n",
      "\n",
      "Epoch 00008: saving model to model_init_2020-12-2013_19_00.364355/model-00008-1.60938-0.19403-1.60787-0.21000.h5\n",
      "Epoch 9/10\n",
      "67/67 [==============================] - 46s 680ms/step - loss: 1.6093 - categorical_accuracy: 0.2289 - val_loss: 1.6082 - val_categorical_accuracy: 0.2100\n",
      "\n",
      "Epoch 00009: saving model to model_init_2020-12-2013_19_00.364355/model-00009-1.60933-0.22886-1.60823-0.21000.h5\n",
      "\n",
      "Epoch 00009: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "Epoch 10/10\n",
      "67/67 [==============================] - 46s 680ms/step - loss: 1.6078 - categorical_accuracy: 0.2239 - val_loss: 1.6083 - val_categorical_accuracy: 0.2100\n",
      "\n",
      "Epoch 00010: saving model to model_init_2020-12-2013_19_00.364355/model-00010-1.60777-0.22388-1.60831-0.21000.h5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f5de9c06668>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit_generator(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1, \n",
    "                    callbacks=callbacks_list, validation_data=val_generator, \n",
    "                    validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The result is 21% on validation set, which very ver less. So, we have to try different model\n",
    "\n",
    "### Model 3:\n",
    "- Using 2 Conv3D layers with filter as 32 then Max Pooling to extract features and dropout of 0.25 to avoid overfitting\n",
    "- Using 2 Conv3D layers with filter as 64 then Max Pooling to extract features and dropout of 0.25 to avoid overfitting\n",
    "- Using 2 Conv3D layers with filter as 64 then Max Pooling to extract features and dropout of 0.25 to avoid overfitting\n",
    "- Kernel Size is (3,3,3)\n",
    "- Feeding it to MLP architecture. One Dense layer with 'Relu', adding batch normalization followed by dropout of 0.5 and final with Softmax activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv3d_17 (Conv3D)           (None, 30, 120, 120, 32)  2624      \n",
      "_________________________________________________________________\n",
      "activation_17 (Activation)   (None, 30, 120, 120, 32)  0         \n",
      "_________________________________________________________________\n",
      "conv3d_18 (Conv3D)           (None, 30, 120, 120, 32)  27680     \n",
      "_________________________________________________________________\n",
      "activation_18 (Activation)   (None, 30, 120, 120, 32)  0         \n",
      "_________________________________________________________________\n",
      "max_pooling3d_15 (MaxPooling (None, 10, 40, 40, 32)    0         \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 10, 40, 40, 32)    0         \n",
      "_________________________________________________________________\n",
      "conv3d_19 (Conv3D)           (None, 10, 40, 40, 64)    55360     \n",
      "_________________________________________________________________\n",
      "activation_19 (Activation)   (None, 10, 40, 40, 64)    0         \n",
      "_________________________________________________________________\n",
      "conv3d_20 (Conv3D)           (None, 10, 40, 40, 64)    110656    \n",
      "_________________________________________________________________\n",
      "activation_20 (Activation)   (None, 10, 40, 40, 64)    0         \n",
      "_________________________________________________________________\n",
      "max_pooling3d_16 (MaxPooling (None, 4, 14, 14, 64)     0         \n",
      "_________________________________________________________________\n",
      "dropout_12 (Dropout)         (None, 4, 14, 14, 64)     0         \n",
      "_________________________________________________________________\n",
      "conv3d_21 (Conv3D)           (None, 4, 14, 14, 64)     110656    \n",
      "_________________________________________________________________\n",
      "activation_21 (Activation)   (None, 4, 14, 14, 64)     0         \n",
      "_________________________________________________________________\n",
      "conv3d_22 (Conv3D)           (None, 4, 14, 14, 64)     110656    \n",
      "_________________________________________________________________\n",
      "activation_22 (Activation)   (None, 4, 14, 14, 64)     0         \n",
      "_________________________________________________________________\n",
      "max_pooling3d_17 (MaxPooling (None, 2, 5, 5, 64)       0         \n",
      "_________________________________________________________________\n",
      "dropout_13 (Dropout)         (None, 2, 5, 5, 64)       0         \n",
      "_________________________________________________________________\n",
      "flatten_5 (Flatten)          (None, 3200)              0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 512)               1638912   \n",
      "_________________________________________________________________\n",
      "batch_normalization_12 (Batc (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "dropout_14 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 5)                 2565      \n",
      "=================================================================\n",
      "Total params: 2,061,157\n",
      "Trainable params: 2,060,133\n",
      "Non-trainable params: 1,024\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Define model\n",
    "model = Sequential()\n",
    "model.add(Conv3D(32, kernel_size=(3, 3, 3), input_shape=(nb_frames,nb_rows,nb_cols,nb_channel), padding=\"same\"))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv3D(32, padding=\"same\", kernel_size=(3, 3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling3D(pool_size=(3, 3, 3), padding=\"same\"))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Conv3D(64, padding=\"same\", kernel_size=(3, 3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv3D(64, padding=\"same\", kernel_size=(3, 3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling3D(pool_size=(3, 3, 3), padding=\"same\"))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Conv3D(64, padding=\"same\", kernel_size=(3, 3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv3D(64, padding=\"same\", kernel_size=(3, 3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling3D(pool_size=(3, 3, 3), padding=\"same\"))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(nb_classes, activation='softmax'))\n",
    "model.compile(optimizer=Adam(), loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- train_generator and val_generator to be used in .fit_generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator = new_generator(train_path, train_doc, batch_size)\n",
    "val_generator = new_generator(val_path, val_doc, batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Fitting the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source path =  ./Project_data/val ; batch size = 10\n",
      "Source path =  ./Project_data/train ; batch size = 10\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/disks/user/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:14: DeprecationWarning: `imread` is deprecated!\n",
      "`imread` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``imageio.imread`` instead.\n",
      "  \n",
      "/mnt/disks/user/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:18: DeprecationWarning: `imresize` is deprecated!\n",
      "`imresize` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``skimage.transform.resize`` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "56/67 [========================>.....] - ETA: 19s - loss: 1.9250 - categorical_accuracy: 0.3411Batch:  67 Index: 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/disks/user/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:38: DeprecationWarning: `imread` is deprecated!\n",
      "`imread` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``imageio.imread`` instead.\n",
      "/mnt/disks/user/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:42: DeprecationWarning: `imresize` is deprecated!\n",
      "`imresize` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``skimage.transform.resize`` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "67/67 [==============================] - 121s 2s/step - loss: 1.9096 - categorical_accuracy: 0.3404 - val_loss: 2.2734 - val_categorical_accuracy: 0.2700\n",
      "\n",
      "Epoch 00001: saving model to model_init_2020-12-2013_19_00.364355/model-00001-1.90882-0.34389-2.27336-0.27000.h5\n",
      "Epoch 2/20\n",
      "67/67 [==============================] - 46s 681ms/step - loss: 1.8820 - categorical_accuracy: 0.2836 - val_loss: 5.9526 - val_categorical_accuracy: 0.2400\n",
      "\n",
      "Epoch 00002: saving model to model_init_2020-12-2013_19_00.364355/model-00002-1.88200-0.28358-5.95262-0.24000.h5\n",
      "Epoch 3/20\n",
      "67/67 [==============================] - 45s 678ms/step - loss: 1.7650 - categorical_accuracy: 0.3184 - val_loss: 2.4464 - val_categorical_accuracy: 0.2700\n",
      "\n",
      "Epoch 00003: saving model to model_init_2020-12-2013_19_00.364355/model-00003-1.76496-0.31841-2.44636-0.27000.h5\n",
      "\n",
      "Epoch 00003: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "Epoch 4/20\n",
      "67/67 [==============================] - 45s 677ms/step - loss: 1.7410 - categorical_accuracy: 0.2786 - val_loss: 1.7797 - val_categorical_accuracy: 0.2800\n",
      "\n",
      "Epoch 00004: saving model to model_init_2020-12-2013_19_00.364355/model-00004-1.74096-0.27861-1.77968-0.28000.h5\n",
      "Epoch 5/20\n",
      "67/67 [==============================] - 45s 677ms/step - loss: 1.6950 - categorical_accuracy: 0.3035 - val_loss: 1.5503 - val_categorical_accuracy: 0.2900\n",
      "\n",
      "Epoch 00005: saving model to model_init_2020-12-2013_19_00.364355/model-00005-1.69504-0.30348-1.55031-0.29000.h5\n",
      "Epoch 6/20\n",
      "67/67 [==============================] - 45s 678ms/step - loss: 1.6470 - categorical_accuracy: 0.3383 - val_loss: 1.4350 - val_categorical_accuracy: 0.3400\n",
      "\n",
      "Epoch 00006: saving model to model_init_2020-12-2013_19_00.364355/model-00006-1.64698-0.33831-1.43502-0.34000.h5\n",
      "Epoch 7/20\n",
      "67/67 [==============================] - 45s 676ms/step - loss: 1.5682 - categorical_accuracy: 0.2935 - val_loss: 1.4413 - val_categorical_accuracy: 0.3900\n",
      "\n",
      "Epoch 00007: saving model to model_init_2020-12-2013_19_00.364355/model-00007-1.56821-0.29353-1.44128-0.39000.h5\n",
      "Epoch 8/20\n",
      "67/67 [==============================] - 45s 677ms/step - loss: 1.5880 - categorical_accuracy: 0.3433 - val_loss: 1.4625 - val_categorical_accuracy: 0.3900\n",
      "\n",
      "Epoch 00008: saving model to model_init_2020-12-2013_19_00.364355/model-00008-1.58798-0.34328-1.46252-0.39000.h5\n",
      "\n",
      "Epoch 00008: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "Epoch 9/20\n",
      "67/67 [==============================] - 45s 674ms/step - loss: 1.5773 - categorical_accuracy: 0.3483 - val_loss: 1.4151 - val_categorical_accuracy: 0.4100\n",
      "\n",
      "Epoch 00009: saving model to model_init_2020-12-2013_19_00.364355/model-00009-1.57728-0.34826-1.41513-0.41000.h5\n",
      "Epoch 10/20\n",
      "67/67 [==============================] - 45s 676ms/step - loss: 1.6004 - categorical_accuracy: 0.3085 - val_loss: 1.5314 - val_categorical_accuracy: 0.3600\n",
      "\n",
      "Epoch 00010: saving model to model_init_2020-12-2013_19_00.364355/model-00010-1.60041-0.30846-1.53139-0.36000.h5\n",
      "Epoch 11/20\n",
      "67/67 [==============================] - 45s 677ms/step - loss: 1.6337 - categorical_accuracy: 0.3284 - val_loss: 1.4447 - val_categorical_accuracy: 0.4000\n",
      "\n",
      "Epoch 00011: saving model to model_init_2020-12-2013_19_00.364355/model-00011-1.63369-0.32836-1.44473-0.40000.h5\n",
      "\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "Epoch 12/20\n",
      "67/67 [==============================] - 45s 677ms/step - loss: 1.5664 - categorical_accuracy: 0.3333 - val_loss: 1.4402 - val_categorical_accuracy: 0.3700\n",
      "\n",
      "Epoch 00012: saving model to model_init_2020-12-2013_19_00.364355/model-00012-1.56642-0.33333-1.44020-0.37000.h5\n",
      "Epoch 13/20\n",
      "67/67 [==============================] - 45s 677ms/step - loss: 1.5577 - categorical_accuracy: 0.3483 - val_loss: 1.4133 - val_categorical_accuracy: 0.3900\n",
      "\n",
      "Epoch 00013: saving model to model_init_2020-12-2013_19_00.364355/model-00013-1.55765-0.34826-1.41326-0.39000.h5\n",
      "Epoch 14/20\n",
      "67/67 [==============================] - 45s 677ms/step - loss: 1.5943 - categorical_accuracy: 0.3234 - val_loss: 1.4188 - val_categorical_accuracy: 0.3800\n",
      "\n",
      "Epoch 00014: saving model to model_init_2020-12-2013_19_00.364355/model-00014-1.59431-0.32338-1.41877-0.38000.h5\n",
      "Epoch 15/20\n",
      "67/67 [==============================] - 45s 677ms/step - loss: 1.6049 - categorical_accuracy: 0.3433 - val_loss: 1.4170 - val_categorical_accuracy: 0.3700\n",
      "\n",
      "Epoch 00015: saving model to model_init_2020-12-2013_19_00.364355/model-00015-1.60488-0.34328-1.41701-0.37000.h5\n",
      "\n",
      "Epoch 00015: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "Epoch 16/20\n",
      "67/67 [==============================] - 45s 677ms/step - loss: 1.5681 - categorical_accuracy: 0.3433 - val_loss: 1.4110 - val_categorical_accuracy: 0.3900\n",
      "\n",
      "Epoch 00016: saving model to model_init_2020-12-2013_19_00.364355/model-00016-1.56805-0.34328-1.41102-0.39000.h5\n",
      "Epoch 17/20\n",
      "67/67 [==============================] - 45s 677ms/step - loss: 1.5406 - categorical_accuracy: 0.3085 - val_loss: 1.3972 - val_categorical_accuracy: 0.3700\n",
      "\n",
      "Epoch 00017: saving model to model_init_2020-12-2013_19_00.364355/model-00017-1.54059-0.30846-1.39725-0.37000.h5\n",
      "Epoch 18/20\n",
      "67/67 [==============================] - 45s 677ms/step - loss: 1.5474 - categorical_accuracy: 0.3433 - val_loss: 1.3930 - val_categorical_accuracy: 0.3900\n",
      "\n",
      "Epoch 00018: saving model to model_init_2020-12-2013_19_00.364355/model-00018-1.54736-0.34328-1.39299-0.39000.h5\n",
      "Epoch 19/20\n",
      "67/67 [==============================] - 45s 678ms/step - loss: 1.4561 - categorical_accuracy: 0.3582 - val_loss: 1.3866 - val_categorical_accuracy: 0.3800\n",
      "\n",
      "Epoch 00019: saving model to model_init_2020-12-2013_19_00.364355/model-00019-1.45615-0.35821-1.38661-0.38000.h5\n",
      "Epoch 20/20\n",
      "67/67 [==============================] - 45s 677ms/step - loss: 1.4704 - categorical_accuracy: 0.4030 - val_loss: 1.3964 - val_categorical_accuracy: 0.3900\n",
      "\n",
      "Epoch 00020: saving model to model_init_2020-12-2013_19_00.364355/model-00020-1.47044-0.40299-1.39645-0.39000.h5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f5deb4d7cc0>"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_epochs = 20\n",
    "model.fit_generator(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1, \n",
    "                    callbacks=callbacks_list, validation_data=val_generator, \n",
    "                    validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The result is 41% on the valdation set, which ver less too. So, creating a new model\n",
    "\n",
    "### Model 4:\n",
    "- Creating a new model with 4 conv3D layers and adding batch activation before each 'relu' activation fucntion, folowed by Max pooling\n",
    "- Adding 2 dense layers with 'relu' activation and 0.5 dropouts and final softmax layer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv3d_23 (Conv3D)           (None, 30, 120, 120, 8)   656       \n",
      "_________________________________________________________________\n",
      "batch_normalization_13 (Batc (None, 30, 120, 120, 8)   32        \n",
      "_________________________________________________________________\n",
      "activation_23 (Activation)   (None, 30, 120, 120, 8)   0         \n",
      "_________________________________________________________________\n",
      "max_pooling3d_18 (MaxPooling (None, 15, 60, 60, 8)     0         \n",
      "_________________________________________________________________\n",
      "conv3d_24 (Conv3D)           (None, 15, 60, 60, 16)    3472      \n",
      "_________________________________________________________________\n",
      "batch_normalization_14 (Batc (None, 15, 60, 60, 16)    64        \n",
      "_________________________________________________________________\n",
      "activation_24 (Activation)   (None, 15, 60, 60, 16)    0         \n",
      "_________________________________________________________________\n",
      "max_pooling3d_19 (MaxPooling (None, 7, 30, 30, 16)     0         \n",
      "_________________________________________________________________\n",
      "conv3d_25 (Conv3D)           (None, 7, 30, 30, 32)     4640      \n",
      "_________________________________________________________________\n",
      "batch_normalization_15 (Batc (None, 7, 30, 30, 32)     128       \n",
      "_________________________________________________________________\n",
      "activation_25 (Activation)   (None, 7, 30, 30, 32)     0         \n",
      "_________________________________________________________________\n",
      "max_pooling3d_20 (MaxPooling (None, 3, 15, 15, 32)     0         \n",
      "_________________________________________________________________\n",
      "conv3d_26 (Conv3D)           (None, 3, 15, 15, 64)     18496     \n",
      "_________________________________________________________________\n",
      "batch_normalization_16 (Batc (None, 3, 15, 15, 64)     256       \n",
      "_________________________________________________________________\n",
      "activation_26 (Activation)   (None, 3, 15, 15, 64)     0         \n",
      "_________________________________________________________________\n",
      "max_pooling3d_21 (MaxPooling (None, 1, 7, 7, 64)       0         \n",
      "_________________________________________________________________\n",
      "flatten_6 (Flatten)          (None, 3136)              0         \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 1000)              3137000   \n",
      "_________________________________________________________________\n",
      "dropout_15 (Dropout)         (None, 1000)              0         \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 500)               500500    \n",
      "_________________________________________________________________\n",
      "dropout_16 (Dropout)         (None, 500)               0         \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 5)                 2505      \n",
      "=================================================================\n",
      "Total params: 3,667,749\n",
      "Trainable params: 3,667,509\n",
      "Non-trainable params: 240\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "input_shape=(nb_frames,nb_rows,nb_cols,nb_channel)\n",
    "\n",
    "# Define model\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Conv3D(nb_filters[0], \n",
    "                 kernel_size=(3,3,3), \n",
    "                 input_shape=input_shape,\n",
    "                 padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(MaxPooling3D(pool_size=(2,2,2)))\n",
    "\n",
    "model.add(Conv3D(nb_filters[1], \n",
    "                 kernel_size=(3,3,3), \n",
    "                 padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(MaxPooling3D(pool_size=(2,2,2)))\n",
    "\n",
    "model.add(Conv3D(nb_filters[2], \n",
    "                 kernel_size=(1,3,3), \n",
    "                 padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(MaxPooling3D(pool_size=(2,2,2)))\n",
    "\n",
    "model.add(Conv3D(nb_filters[3], \n",
    "                 kernel_size=(1,3,3), \n",
    "                 padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(MaxPooling3D(pool_size=(2,2,2)))\n",
    "\n",
    "#Flatten Layers\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(nb_dense[0], activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(nb_dense[1], activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "#softmax layer\n",
    "model.add(Dense(nb_dense[2], activation='softmax'))\n",
    "model.compile(optimizer=Adam(), loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- fitting the model using train_generator and val_generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source path =  ./Project_data/val ; batch size = 10\n",
      "Source path =  ./Project_data/trainEpoch 1/20\n",
      " ; batch size = 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/disks/user/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:14: DeprecationWarning: `imread` is deprecated!\n",
      "`imread` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``imageio.imread`` instead.\n",
      "  \n",
      "/mnt/disks/user/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:18: DeprecationWarning: `imresize` is deprecated!\n",
      "`imresize` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``skimage.transform.resize`` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65/67 [============================>.] - ETA: 2s - loss: 5.3494 - categorical_accuracy: 0.3046Batch:  67 Index: 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/disks/user/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:38: DeprecationWarning: `imread` is deprecated!\n",
      "`imread` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``imageio.imread`` instead.\n",
      "/mnt/disks/user/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:42: DeprecationWarning: `imresize` is deprecated!\n",
      "`imresize` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``skimage.transform.resize`` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "67/67 [==============================] - 86s 1s/step - loss: 5.2480 - categorical_accuracy: 0.3069 - val_loss: 5.9549 - val_categorical_accuracy: 0.2500\n",
      "\n",
      "Epoch 00001: saving model to model_init_2020-12-2013_19_00.364355/model-00001-5.28812-0.30317-5.95491-0.25000.h5\n",
      "Epoch 2/20\n",
      "67/67 [==============================] - 24s 361ms/step - loss: 2.2719 - categorical_accuracy: 0.3134 - val_loss: 3.1975 - val_categorical_accuracy: 0.2800\n",
      "\n",
      "Epoch 00002: saving model to model_init_2020-12-2013_19_00.364355/model-00002-2.27186-0.31343-3.19750-0.28000.h5\n",
      "Epoch 3/20\n",
      "67/67 [==============================] - 27s 407ms/step - loss: 1.7420 - categorical_accuracy: 0.3134 - val_loss: 1.4169 - val_categorical_accuracy: 0.3200\n",
      "\n",
      "Epoch 00003: saving model to model_init_2020-12-2013_19_00.364355/model-00003-1.74199-0.31343-1.41688-0.32000.h5\n",
      "Epoch 4/20\n",
      "67/67 [==============================] - 29s 428ms/step - loss: 1.6395 - categorical_accuracy: 0.3035 - val_loss: 1.4706 - val_categorical_accuracy: 0.3600\n",
      "\n",
      "Epoch 00004: saving model to model_init_2020-12-2013_19_00.364355/model-00004-1.63951-0.30348-1.47064-0.36000.h5\n",
      "Epoch 5/20\n",
      "67/67 [==============================] - 28s 423ms/step - loss: 1.5844 - categorical_accuracy: 0.3383 - val_loss: 1.2243 - val_categorical_accuracy: 0.4200\n",
      "\n",
      "Epoch 00005: saving model to model_init_2020-12-2013_19_00.364355/model-00005-1.58442-0.33831-1.22428-0.42000.h5\n",
      "Epoch 6/20\n",
      "67/67 [==============================] - 28s 413ms/step - loss: 1.4006 - categorical_accuracy: 0.3930 - val_loss: 1.1931 - val_categorical_accuracy: 0.4300\n",
      "\n",
      "Epoch 00006: saving model to model_init_2020-12-2013_19_00.364355/model-00006-1.40059-0.39303-1.19306-0.43000.h5\n",
      "Epoch 7/20\n",
      "67/67 [==============================] - 28s 411ms/step - loss: 1.4847 - categorical_accuracy: 0.3532 - val_loss: 1.2156 - val_categorical_accuracy: 0.5000\n",
      "\n",
      "Epoch 00007: saving model to model_init_2020-12-2013_19_00.364355/model-00007-1.48468-0.35323-1.21557-0.50000.h5\n",
      "Epoch 8/20\n",
      "67/67 [==============================] - 28s 422ms/step - loss: 1.5476 - categorical_accuracy: 0.3483 - val_loss: 1.3945 - val_categorical_accuracy: 0.4400\n",
      "\n",
      "Epoch 00008: saving model to model_init_2020-12-2013_19_00.364355/model-00008-1.54764-0.34826-1.39450-0.44000.h5\n",
      "\n",
      "Epoch 00008: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "Epoch 9/20\n",
      "67/67 [==============================] - 27s 407ms/step - loss: 1.3445 - categorical_accuracy: 0.3532 - val_loss: 1.0640 - val_categorical_accuracy: 0.5400\n",
      "\n",
      "Epoch 00009: saving model to model_init_2020-12-2013_19_00.364355/model-00009-1.34453-0.35323-1.06396-0.54000.h5\n",
      "Epoch 10/20\n",
      "67/67 [==============================] - 27s 405ms/step - loss: 1.3264 - categorical_accuracy: 0.3980 - val_loss: 1.0383 - val_categorical_accuracy: 0.5300\n",
      "\n",
      "Epoch 00010: saving model to model_init_2020-12-2013_19_00.364355/model-00010-1.32635-0.39801-1.03828-0.53000.h5\n",
      "Epoch 11/20\n",
      "67/67 [==============================] - 27s 405ms/step - loss: 1.3850 - categorical_accuracy: 0.4229 - val_loss: 1.0214 - val_categorical_accuracy: 0.5400\n",
      "\n",
      "Epoch 00011: saving model to model_init_2020-12-2013_19_00.364355/model-00011-1.38499-0.42289-1.02138-0.54000.h5\n",
      "Epoch 12/20\n",
      "67/67 [==============================] - 27s 409ms/step - loss: 1.3527 - categorical_accuracy: 0.3831 - val_loss: 1.1108 - val_categorical_accuracy: 0.5400\n",
      "\n",
      "Epoch 00012: saving model to model_init_2020-12-2013_19_00.364355/model-00012-1.35269-0.38308-1.11078-0.54000.h5\n",
      "Epoch 13/20\n",
      "67/67 [==============================] - 27s 398ms/step - loss: 1.2572 - categorical_accuracy: 0.4378 - val_loss: 1.0308 - val_categorical_accuracy: 0.5800\n",
      "\n",
      "Epoch 00013: saving model to model_init_2020-12-2013_19_00.364355/model-00013-1.25718-0.43781-1.03077-0.58000.h5\n",
      "\n",
      "Epoch 00013: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "Epoch 14/20\n",
      "67/67 [==============================] - 28s 415ms/step - loss: 1.2947 - categorical_accuracy: 0.4129 - val_loss: 1.0072 - val_categorical_accuracy: 0.5900\n",
      "\n",
      "Epoch 00014: saving model to model_init_2020-12-2013_19_00.364355/model-00014-1.29472-0.41294-1.00723-0.59000.h5\n",
      "Epoch 15/20\n",
      "67/67 [==============================] - 28s 414ms/step - loss: 1.2686 - categorical_accuracy: 0.4179 - val_loss: 0.9760 - val_categorical_accuracy: 0.6500\n",
      "\n",
      "Epoch 00015: saving model to model_init_2020-12-2013_19_00.364355/model-00015-1.26864-0.41791-0.97596-0.65000.h5\n",
      "Epoch 16/20\n",
      "67/67 [==============================] - 27s 409ms/step - loss: 1.0897 - categorical_accuracy: 0.5025 - val_loss: 0.9427 - val_categorical_accuracy: 0.6100\n",
      "\n",
      "Epoch 00016: saving model to model_init_2020-12-2013_19_00.364355/model-00016-1.08974-0.50249-0.94270-0.61000.h5\n",
      "Epoch 17/20\n",
      "67/67 [==============================] - 27s 403ms/step - loss: 1.2146 - categorical_accuracy: 0.4577 - val_loss: 0.9318 - val_categorical_accuracy: 0.6300\n",
      "\n",
      "Epoch 00017: saving model to model_init_2020-12-2013_19_00.364355/model-00017-1.21458-0.45771-0.93176-0.63000.h5\n",
      "Epoch 18/20\n",
      "67/67 [==============================] - 28s 420ms/step - loss: 1.2028 - categorical_accuracy: 0.4975 - val_loss: 0.9174 - val_categorical_accuracy: 0.6600\n",
      "\n",
      "Epoch 00018: saving model to model_init_2020-12-2013_19_00.364355/model-00018-1.20282-0.49751-0.91735-0.66000.h5\n",
      "Epoch 19/20\n",
      "67/67 [==============================] - 26s 395ms/step - loss: 1.1913 - categorical_accuracy: 0.5025 - val_loss: 0.9929 - val_categorical_accuracy: 0.6400\n",
      "\n",
      "Epoch 00019: saving model to model_init_2020-12-2013_19_00.364355/model-00019-1.19127-0.50249-0.99290-0.64000.h5\n",
      "Epoch 20/20\n",
      "67/67 [==============================] - 27s 403ms/step - loss: 1.1392 - categorical_accuracy: 0.5224 - val_loss: 0.8688 - val_categorical_accuracy: 0.6700\n",
      "\n",
      "Epoch 00020: saving model to model_init_2020-12-2013_19_00.364355/model-00020-1.13918-0.52239-0.86878-0.67000.h5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f5de9e6cb38>"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_generator = new_generator(train_path, train_doc, batch_size)\n",
    "val_generator = new_generator(val_path, val_doc, batch_size)\n",
    "num_epochs = 20\n",
    "model.fit_generator(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1, \n",
    "                    callbacks=callbacks_list, validation_data=val_generator, \n",
    "                    validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The valiadtion accuracy is 67%, which is lot more than what we were getting till now. But let us try if it can be increased further"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_frames = 30 # number of frames\n",
    "nb_rows = 64 # image width\n",
    "nb_cols = 64 # image height \n",
    "\n",
    "nb_classes = 5\n",
    "nb_channel = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 5:\n",
    "Creating a new model by changing a few layers  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv3d_27 (Conv3D)           (None, 30, 64, 64, 32)    896       \n",
      "_________________________________________________________________\n",
      "activation_27 (Activation)   (None, 30, 64, 64, 32)    0         \n",
      "_________________________________________________________________\n",
      "conv3d_28 (Conv3D)           (None, 30, 64, 64, 32)    27680     \n",
      "_________________________________________________________________\n",
      "activation_28 (Activation)   (None, 30, 64, 64, 32)    0         \n",
      "_________________________________________________________________\n",
      "max_pooling3d_22 (MaxPooling (None, 10, 22, 22, 32)    0         \n",
      "_________________________________________________________________\n",
      "dropout_17 (Dropout)         (None, 10, 22, 22, 32)    0         \n",
      "_________________________________________________________________\n",
      "conv3d_29 (Conv3D)           (None, 10, 22, 22, 64)    55360     \n",
      "_________________________________________________________________\n",
      "activation_29 (Activation)   (None, 10, 22, 22, 64)    0         \n",
      "_________________________________________________________________\n",
      "conv3d_30 (Conv3D)           (None, 10, 22, 22, 64)    110656    \n",
      "_________________________________________________________________\n",
      "activation_30 (Activation)   (None, 10, 22, 22, 64)    0         \n",
      "_________________________________________________________________\n",
      "max_pooling3d_23 (MaxPooling (None, 4, 8, 8, 64)       0         \n",
      "_________________________________________________________________\n",
      "dropout_18 (Dropout)         (None, 4, 8, 8, 64)       0         \n",
      "_________________________________________________________________\n",
      "conv3d_31 (Conv3D)           (None, 4, 8, 8, 64)       110656    \n",
      "_________________________________________________________________\n",
      "activation_31 (Activation)   (None, 4, 8, 8, 64)       0         \n",
      "_________________________________________________________________\n",
      "conv3d_32 (Conv3D)           (None, 4, 8, 8, 64)       110656    \n",
      "_________________________________________________________________\n",
      "activation_32 (Activation)   (None, 4, 8, 8, 64)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling3d_24 (MaxPooling (None, 2, 3, 3, 64)       0         \n",
      "_________________________________________________________________\n",
      "dropout_19 (Dropout)         (None, 2, 3, 3, 64)       0         \n",
      "_________________________________________________________________\n",
      "flatten_7 (Flatten)          (None, 1152)              0         \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 512)               590336    \n",
      "_________________________________________________________________\n",
      "batch_normalization_17 (Batc (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "dropout_20 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 5)                 2565      \n",
      "=================================================================\n",
      "Total params: 1,010,853\n",
      "Trainable params: 1,009,829\n",
      "Non-trainable params: 1,024\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Define model\n",
    "model = Sequential()\n",
    "model.add(Conv3D(32, kernel_size=(3, 3, 3), input_shape=(nb_frames,nb_rows,nb_cols,nb_channel), padding=\"same\"))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv3D(32, padding=\"same\", kernel_size=(3, 3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling3D(pool_size=(3, 3, 3), padding=\"same\"))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Conv3D(64, padding=\"same\", kernel_size=(3, 3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv3D(64, padding=\"same\", kernel_size=(3, 3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling3D(pool_size=(3, 3, 3), padding=\"same\"))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Conv3D(64, padding=\"same\", kernel_size=(3, 3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv3D(64, padding=\"same\", kernel_size=(3, 3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling3D(pool_size=(3, 3, 3), padding=\"same\"))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(nb_classes, activation='softmax'))\n",
    "model.compile(optimizer=Adam(), loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Fitting the model with batch size 20 and number of epochs as 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source path =  Source path =  ./Project_data/train ; batch size = 20\n",
      "Epoch 1/20\n",
      "./Project_data/val ; batch size = 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/disks/user/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:14: DeprecationWarning: `imread` is deprecated!\n",
      "`imread` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``imageio.imread`` instead.\n",
      "  \n",
      "/mnt/disks/user/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:18: DeprecationWarning: `imresize` is deprecated!\n",
      "`imresize` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``skimage.transform.resize`` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/67 [=============>................] - ETA: 1:18 - loss: 1.6882 - categorical_accuracy: 0.3156Batch:  34 Index: 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/disks/user/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:38: DeprecationWarning: `imread` is deprecated!\n",
      "`imread` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``imageio.imread`` instead.\n",
      "/mnt/disks/user/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:42: DeprecationWarning: `imresize` is deprecated!\n",
      "`imresize` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``skimage.transform.resize`` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "67/67 [==============================] - 85s 1s/step - loss: 1.5791 - categorical_accuracy: 0.3496 - val_loss: 3.0353 - val_categorical_accuracy: 0.1600\n",
      "\n",
      "Epoch 00001: saving model to model_init_2020-12-2013_19_00.364355/model-00001-1.66145-0.32546-3.03525-0.16000.h5\n",
      "Epoch 2/20\n",
      "67/67 [==============================] - 26s 393ms/step - loss: 1.6166 - categorical_accuracy: 0.2537 - val_loss: 1.7599 - val_categorical_accuracy: 0.2300\n",
      "\n",
      "Epoch 00002: saving model to model_init_2020-12-2013_19_00.364355/model-00002-1.61662-0.25373-1.75995-0.23000.h5\n",
      "Epoch 3/20\n",
      "67/67 [==============================] - 30s 445ms/step - loss: 1.5356 - categorical_accuracy: 0.3383 - val_loss: 3.5339 - val_categorical_accuracy: 0.1600\n",
      "\n",
      "Epoch 00003: saving model to model_init_2020-12-2013_19_00.364355/model-00003-1.53556-0.33831-3.53389-0.16000.h5\n",
      "Epoch 4/20\n",
      "67/67 [==============================] - 30s 441ms/step - loss: 1.5494 - categorical_accuracy: 0.3085 - val_loss: 1.5636 - val_categorical_accuracy: 0.2300\n",
      "\n",
      "Epoch 00004: saving model to model_init_2020-12-2013_19_00.364355/model-00004-1.54942-0.30846-1.56355-0.23000.h5\n",
      "Epoch 5/20\n",
      "67/67 [==============================] - 29s 440ms/step - loss: 1.5027 - categorical_accuracy: 0.3582 - val_loss: 3.0200 - val_categorical_accuracy: 0.2500\n",
      "\n",
      "Epoch 00005: saving model to model_init_2020-12-2013_19_00.364355/model-00005-1.50267-0.35821-3.02001-0.25000.h5\n",
      "Epoch 6/20\n",
      "67/67 [==============================] - 29s 439ms/step - loss: 1.5759 - categorical_accuracy: 0.3234 - val_loss: 1.5227 - val_categorical_accuracy: 0.3600\n",
      "\n",
      "Epoch 00006: saving model to model_init_2020-12-2013_19_00.364355/model-00006-1.57588-0.32338-1.52270-0.36000.h5\n",
      "Epoch 7/20\n",
      "67/67 [==============================] - 29s 438ms/step - loss: 1.6321 - categorical_accuracy: 0.2587 - val_loss: 2.4504 - val_categorical_accuracy: 0.2100\n",
      "\n",
      "Epoch 00007: saving model to model_init_2020-12-2013_19_00.364355/model-00007-1.63210-0.25871-2.45044-0.21000.h5\n",
      "Epoch 8/20\n",
      "67/67 [==============================] - 29s 438ms/step - loss: 1.6288 - categorical_accuracy: 0.2239 - val_loss: 2.1818 - val_categorical_accuracy: 0.1600\n",
      "\n",
      "Epoch 00008: saving model to model_init_2020-12-2013_19_00.364355/model-00008-1.62875-0.22388-2.18179-0.16000.h5\n",
      "\n",
      "Epoch 00008: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "Epoch 9/20\n",
      "67/67 [==============================] - 28s 424ms/step - loss: 1.5992 - categorical_accuracy: 0.2438 - val_loss: 1.9557 - val_categorical_accuracy: 0.1800\n",
      "\n",
      "Epoch 00009: saving model to model_init_2020-12-2013_19_00.364355/model-00009-1.59917-0.24378-1.95571-0.18000.h5\n",
      "Epoch 10/20\n",
      "67/67 [==============================] - 29s 437ms/step - loss: 1.5484 - categorical_accuracy: 0.3184 - val_loss: 1.3217 - val_categorical_accuracy: 0.4000\n",
      "\n",
      "Epoch 00010: saving model to model_init_2020-12-2013_19_00.364355/model-00010-1.54845-0.31841-1.32166-0.40000.h5\n",
      "Epoch 11/20\n",
      "67/67 [==============================] - 29s 430ms/step - loss: 1.4009 - categorical_accuracy: 0.3831 - val_loss: 1.2749 - val_categorical_accuracy: 0.4200\n",
      "\n",
      "Epoch 00011: saving model to model_init_2020-12-2013_19_00.364355/model-00011-1.40091-0.38308-1.27489-0.42000.h5\n",
      "Epoch 12/20\n",
      "67/67 [==============================] - 29s 435ms/step - loss: 1.3814 - categorical_accuracy: 0.4030 - val_loss: 1.3382 - val_categorical_accuracy: 0.3900\n",
      "\n",
      "Epoch 00012: saving model to model_init_2020-12-2013_19_00.364355/model-00012-1.38143-0.40299-1.33819-0.39000.h5\n",
      "Epoch 13/20\n",
      "67/67 [==============================] - 29s 435ms/step - loss: 1.4488 - categorical_accuracy: 0.3483 - val_loss: 1.2036 - val_categorical_accuracy: 0.4100\n",
      "\n",
      "Epoch 00013: saving model to model_init_2020-12-2013_19_00.364355/model-00013-1.44878-0.34826-1.20361-0.41000.h5\n",
      "Epoch 14/20\n",
      "67/67 [==============================] - 29s 434ms/step - loss: 1.3885 - categorical_accuracy: 0.3881 - val_loss: 1.4034 - val_categorical_accuracy: 0.2800\n",
      "\n",
      "Epoch 00014: saving model to model_init_2020-12-2013_19_00.364355/model-00014-1.38849-0.38806-1.40338-0.28000.h5\n",
      "Epoch 15/20\n",
      "67/67 [==============================] - 30s 443ms/step - loss: 1.4166 - categorical_accuracy: 0.3731 - val_loss: 1.4525 - val_categorical_accuracy: 0.3800\n",
      "\n",
      "Epoch 00015: saving model to model_init_2020-12-2013_19_00.364355/model-00015-1.41662-0.37313-1.45255-0.38000.h5\n",
      "\n",
      "Epoch 00015: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "Epoch 16/20\n",
      "67/67 [==============================] - 29s 436ms/step - loss: 1.3668 - categorical_accuracy: 0.4428 - val_loss: 1.2480 - val_categorical_accuracy: 0.4600\n",
      "\n",
      "Epoch 00016: saving model to model_init_2020-12-2013_19_00.364355/model-00016-1.36681-0.44279-1.24795-0.46000.h5\n",
      "Epoch 17/20\n",
      "67/67 [==============================] - 29s 431ms/step - loss: 1.4049 - categorical_accuracy: 0.3930 - val_loss: 1.3279 - val_categorical_accuracy: 0.3900\n",
      "\n",
      "Epoch 00017: saving model to model_init_2020-12-2013_19_00.364355/model-00017-1.40492-0.39303-1.32789-0.39000.h5\n",
      "\n",
      "Epoch 00017: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "Epoch 18/20\n",
      "67/67 [==============================] - 28s 424ms/step - loss: 1.3336 - categorical_accuracy: 0.4478 - val_loss: 1.1270 - val_categorical_accuracy: 0.4400\n",
      "\n",
      "Epoch 00018: saving model to model_init_2020-12-2013_19_00.364355/model-00018-1.33364-0.44776-1.12695-0.44000.h5\n",
      "Epoch 19/20\n",
      "67/67 [==============================] - 29s 435ms/step - loss: 1.3354 - categorical_accuracy: 0.3682 - val_loss: 1.0962 - val_categorical_accuracy: 0.5100\n",
      "\n",
      "Epoch 00019: saving model to model_init_2020-12-2013_19_00.364355/model-00019-1.33542-0.36816-1.09621-0.51000.h5\n",
      "Epoch 20/20\n",
      "67/67 [==============================] - 29s 428ms/step - loss: 1.3131 - categorical_accuracy: 0.4129 - val_loss: 1.0985 - val_categorical_accuracy: 0.5000\n",
      "\n",
      "Epoch 00020: saving model to model_init_2020-12-2013_19_00.364355/model-00020-1.31306-0.41294-1.09847-0.50000.h5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f5de7062ac8>"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 20\n",
    "train_generator = new_generator(train_path, train_doc, batch_size)\n",
    "val_generator = new_generator(val_path, val_doc, batch_size)\n",
    "num_epochs = 20\n",
    "model.fit_generator(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1, \n",
    "                    callbacks=callbacks_list, validation_data=val_generator, \n",
    "                    validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The validation accuracy is 51% and it is still not saticfactory. \n",
    "\n",
    "### Model 6:\n",
    "- Trying out the previous model with batch normalization after every conv3D layer but this time with batch size 10 and epochs as 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv3d_9 (Conv3D)            (None, 30, 120, 120, 8)   224       \n",
      "_________________________________________________________________\n",
      "batch_normalization_9 (Batch (None, 30, 120, 120, 8)   32        \n",
      "_________________________________________________________________\n",
      "activation_9 (Activation)    (None, 30, 120, 120, 8)   0         \n",
      "_________________________________________________________________\n",
      "max_pooling3d_9 (MaxPooling3 (None, 15, 60, 60, 8)     0         \n",
      "_________________________________________________________________\n",
      "conv3d_10 (Conv3D)           (None, 15, 60, 60, 16)    3472      \n",
      "_________________________________________________________________\n",
      "batch_normalization_10 (Batc (None, 15, 60, 60, 16)    64        \n",
      "_________________________________________________________________\n",
      "activation_10 (Activation)   (None, 15, 60, 60, 16)    0         \n",
      "_________________________________________________________________\n",
      "max_pooling3d_10 (MaxPooling (None, 7, 30, 30, 16)     0         \n",
      "_________________________________________________________________\n",
      "conv3d_11 (Conv3D)           (None, 7, 30, 30, 32)     4640      \n",
      "_________________________________________________________________\n",
      "batch_normalization_11 (Batc (None, 7, 30, 30, 32)     128       \n",
      "_________________________________________________________________\n",
      "activation_11 (Activation)   (None, 7, 30, 30, 32)     0         \n",
      "_________________________________________________________________\n",
      "max_pooling3d_11 (MaxPooling (None, 3, 15, 15, 32)     0         \n",
      "_________________________________________________________________\n",
      "conv3d_12 (Conv3D)           (None, 3, 15, 15, 64)     18496     \n",
      "_________________________________________________________________\n",
      "batch_normalization_12 (Batc (None, 3, 15, 15, 64)     256       \n",
      "_________________________________________________________________\n",
      "activation_12 (Activation)   (None, 3, 15, 15, 64)     0         \n",
      "_________________________________________________________________\n",
      "max_pooling3d_12 (MaxPooling (None, 1, 7, 7, 64)       0         \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 3136)              0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 1000)              3137000   \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 1000)              0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 500)               500500    \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 500)               0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 5)                 2505      \n",
      "=================================================================\n",
      "Total params: 3,667,317\n",
      "Trainable params: 3,667,077\n",
      "Non-trainable params: 240\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "nb_frames = 30 # number of frames\n",
    "nb_rows = 120 # image width\n",
    "nb_cols = 120 # image height \n",
    "\n",
    "nb_classes = 5\n",
    "nb_channel = 1\n",
    "\n",
    "input_shape=(nb_frames,nb_rows,nb_cols,nb_channel)\n",
    "\n",
    "# Define model\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Conv3D(nb_filters[0], \n",
    "                 kernel_size=(3,3,3), \n",
    "                 input_shape=input_shape,\n",
    "                 padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(MaxPooling3D(pool_size=(2,2,2)))\n",
    "\n",
    "model.add(Conv3D(nb_filters[1], \n",
    "                 kernel_size=(3,3,3), \n",
    "                 padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(MaxPooling3D(pool_size=(2,2,2)))\n",
    "\n",
    "model.add(Conv3D(nb_filters[2], \n",
    "                 kernel_size=(1,3,3), \n",
    "                 padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(MaxPooling3D(pool_size=(2,2,2)))\n",
    "\n",
    "model.add(Conv3D(nb_filters[3], \n",
    "                 kernel_size=(1,3,3), \n",
    "                 padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(MaxPooling3D(pool_size=(2,2,2)))\n",
    "\n",
    "#Flatten Layers\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(nb_dense[0], activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(nb_dense[1], activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "#softmax layer\n",
    "model.add(Dense(nb_dense[2], activation='softmax'))\n",
    "model.compile(optimizer=Adam(), loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Fitting the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source path =  ./Project_data/val ; batch size = 10\n",
      "Source path =  ./Project_data/train ; batch size = 10\n",
      "Epoch 1/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/disks/user/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:14: DeprecationWarning: `imread` is deprecated!\n",
      "`imread` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``imageio.imread`` instead.\n",
      "  \n",
      "/mnt/disks/user/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:18: DeprecationWarning: `imresize` is deprecated!\n",
      "`imresize` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``skimage.transform.resize`` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65/67 [============================>.] - ETA: 4s - loss: 5.6466 - categorical_accuracy: 0.2985Batch:  67 Index: 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/disks/user/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:38: DeprecationWarning: `imread` is deprecated!\n",
      "`imread` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``imageio.imread`` instead.\n",
      "/mnt/disks/user/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:42: DeprecationWarning: `imresize` is deprecated!\n",
      "`imresize` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``skimage.transform.resize`` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "67/67 [==============================] - 143s 2s/step - loss: 5.6058 - categorical_accuracy: 0.2975 - val_loss: 5.6025 - val_categorical_accuracy: 0.2800\n",
      "\n",
      "Epoch 00001: saving model to model_init_2020-12-2104_40_49.443651/model-00001-5.61891-0.29713-5.60247-0.28000.h5\n",
      "Epoch 2/30\n",
      "67/67 [==============================] - 24s 363ms/step - loss: 2.5152 - categorical_accuracy: 0.3433 - val_loss: 1.4799 - val_categorical_accuracy: 0.3900\n",
      "\n",
      "Epoch 00002: saving model to model_init_2020-12-2104_40_49.443651/model-00002-2.51518-0.34328-1.47995-0.39000.h5\n",
      "Epoch 3/30\n",
      "67/67 [==============================] - 23s 349ms/step - loss: 1.7355 - categorical_accuracy: 0.3333 - val_loss: 1.2374 - val_categorical_accuracy: 0.4400\n",
      "\n",
      "Epoch 00003: saving model to model_init_2020-12-2104_40_49.443651/model-00003-1.73550-0.33333-1.23735-0.44000.h5\n",
      "Epoch 4/30\n",
      "67/67 [==============================] - 26s 381ms/step - loss: 1.5360 - categorical_accuracy: 0.3383 - val_loss: 1.3706 - val_categorical_accuracy: 0.3500\n",
      "\n",
      "Epoch 00004: saving model to model_init_2020-12-2104_40_49.443651/model-00004-1.53595-0.33831-1.37061-0.35000.h5\n",
      "Epoch 5/30\n",
      "67/67 [==============================] - 26s 384ms/step - loss: 1.5265 - categorical_accuracy: 0.3930 - val_loss: 1.1153 - val_categorical_accuracy: 0.5000\n",
      "\n",
      "Epoch 00005: saving model to model_init_2020-12-2104_40_49.443651/model-00005-1.52653-0.39303-1.11527-0.50000.h5\n",
      "Epoch 6/30\n",
      "67/67 [==============================] - 25s 373ms/step - loss: 1.5315 - categorical_accuracy: 0.3184 - val_loss: 1.3699 - val_categorical_accuracy: 0.3900\n",
      "\n",
      "Epoch 00006: saving model to model_init_2020-12-2104_40_49.443651/model-00006-1.53150-0.31841-1.36991-0.39000.h5\n",
      "Epoch 7/30\n",
      "67/67 [==============================] - 23s 344ms/step - loss: 1.4464 - categorical_accuracy: 0.3682 - val_loss: 1.3144 - val_categorical_accuracy: 0.4200\n",
      "\n",
      "Epoch 00007: saving model to model_init_2020-12-2104_40_49.443651/model-00007-1.44645-0.36816-1.31438-0.42000.h5\n",
      "\n",
      "Epoch 00007: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "Epoch 8/30\n",
      "67/67 [==============================] - 24s 361ms/step - loss: 1.3956 - categorical_accuracy: 0.3731 - val_loss: 1.1111 - val_categorical_accuracy: 0.5200\n",
      "\n",
      "Epoch 00008: saving model to model_init_2020-12-2104_40_49.443651/model-00008-1.39563-0.37313-1.11105-0.52000.h5\n",
      "Epoch 9/30\n",
      "67/67 [==============================] - 25s 375ms/step - loss: 1.3591 - categorical_accuracy: 0.4328 - val_loss: 1.0576 - val_categorical_accuracy: 0.5300\n",
      "\n",
      "Epoch 00009: saving model to model_init_2020-12-2104_40_49.443651/model-00009-1.35909-0.43284-1.05758-0.53000.h5\n",
      "Epoch 10/30\n",
      "67/67 [==============================] - 25s 367ms/step - loss: 1.2945 - categorical_accuracy: 0.4279 - val_loss: 1.1610 - val_categorical_accuracy: 0.5100\n",
      "\n",
      "Epoch 00010: saving model to model_init_2020-12-2104_40_49.443651/model-00010-1.29452-0.42786-1.16099-0.51000.h5\n",
      "Epoch 11/30\n",
      "67/67 [==============================] - 24s 355ms/step - loss: 1.3350 - categorical_accuracy: 0.4229 - val_loss: 1.1681 - val_categorical_accuracy: 0.5500\n",
      "\n",
      "Epoch 00011: saving model to model_init_2020-12-2104_40_49.443651/model-00011-1.33497-0.42289-1.16807-0.55000.h5\n",
      "\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "Epoch 12/30\n",
      "67/67 [==============================] - 24s 363ms/step - loss: 1.3107 - categorical_accuracy: 0.4229 - val_loss: 1.0609 - val_categorical_accuracy: 0.5200\n",
      "\n",
      "Epoch 00012: saving model to model_init_2020-12-2104_40_49.443651/model-00012-1.31075-0.42289-1.06088-0.52000.h5\n",
      "Epoch 13/30\n",
      "67/67 [==============================] - 24s 360ms/step - loss: 1.1857 - categorical_accuracy: 0.5075 - val_loss: 0.9914 - val_categorical_accuracy: 0.5600\n",
      "\n",
      "Epoch 00013: saving model to model_init_2020-12-2104_40_49.443651/model-00013-1.18573-0.50746-0.99142-0.56000.h5\n",
      "Epoch 14/30\n",
      "67/67 [==============================] - 25s 378ms/step - loss: 1.1413 - categorical_accuracy: 0.5323 - val_loss: 0.9663 - val_categorical_accuracy: 0.6500\n",
      "\n",
      "Epoch 00014: saving model to model_init_2020-12-2104_40_49.443651/model-00014-1.14132-0.53234-0.96631-0.65000.h5\n",
      "Epoch 15/30\n",
      "67/67 [==============================] - 24s 357ms/step - loss: 1.1300 - categorical_accuracy: 0.5373 - val_loss: 1.0195 - val_categorical_accuracy: 0.5900\n",
      "\n",
      "Epoch 00015: saving model to model_init_2020-12-2104_40_49.443651/model-00015-1.13002-0.53731-1.01951-0.59000.h5\n",
      "Epoch 16/30\n",
      "67/67 [==============================] - 24s 351ms/step - loss: 1.1399 - categorical_accuracy: 0.5124 - val_loss: 0.9361 - val_categorical_accuracy: 0.5300\n",
      "\n",
      "Epoch 00016: saving model to model_init_2020-12-2104_40_49.443651/model-00016-1.13989-0.51244-0.93607-0.53000.h5\n",
      "Epoch 17/30\n",
      "67/67 [==============================] - 25s 366ms/step - loss: 1.0739 - categorical_accuracy: 0.5622 - val_loss: 0.9479 - val_categorical_accuracy: 0.5000\n",
      "\n",
      "Epoch 00017: saving model to model_init_2020-12-2104_40_49.443651/model-00017-1.07389-0.56219-0.94794-0.50000.h5\n",
      "Epoch 18/30\n",
      "67/67 [==============================] - 23s 338ms/step - loss: 1.0809 - categorical_accuracy: 0.5473 - val_loss: 1.0182 - val_categorical_accuracy: 0.5900\n",
      "\n",
      "Epoch 00018: saving model to model_init_2020-12-2104_40_49.443651/model-00018-1.08092-0.54726-1.01818-0.59000.h5\n",
      "\n",
      "Epoch 00018: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "Epoch 19/30\n",
      "67/67 [==============================] - 24s 364ms/step - loss: 0.9844 - categorical_accuracy: 0.5871 - val_loss: 0.9014 - val_categorical_accuracy: 0.6200\n",
      "\n",
      "Epoch 00019: saving model to model_init_2020-12-2104_40_49.443651/model-00019-0.98444-0.58706-0.90143-0.62000.h5\n",
      "Epoch 20/30\n",
      "67/67 [==============================] - 24s 356ms/step - loss: 1.0509 - categorical_accuracy: 0.5323 - val_loss: 0.9181 - val_categorical_accuracy: 0.5500\n",
      "\n",
      "Epoch 00020: saving model to model_init_2020-12-2104_40_49.443651/model-00020-1.05094-0.53234-0.91805-0.55000.h5\n",
      "Epoch 21/30\n",
      "67/67 [==============================] - 24s 357ms/step - loss: 1.0446 - categorical_accuracy: 0.5522 - val_loss: 0.8876 - val_categorical_accuracy: 0.5800\n",
      "\n",
      "Epoch 00021: saving model to model_init_2020-12-2104_40_49.443651/model-00021-1.04465-0.55224-0.88762-0.58000.h5\n",
      "Epoch 22/30\n",
      "67/67 [==============================] - 23s 350ms/step - loss: 0.9273 - categorical_accuracy: 0.6119 - val_loss: 0.8717 - val_categorical_accuracy: 0.5800\n",
      "\n",
      "Epoch 00022: saving model to model_init_2020-12-2104_40_49.443651/model-00022-0.92731-0.61194-0.87169-0.58000.h5\n",
      "Epoch 23/30\n",
      "67/67 [==============================] - 24s 355ms/step - loss: 0.9959 - categorical_accuracy: 0.5522 - val_loss: 0.9017 - val_categorical_accuracy: 0.6000\n",
      "\n",
      "Epoch 00023: saving model to model_init_2020-12-2104_40_49.443651/model-00023-0.99588-0.55224-0.90168-0.60000.h5\n",
      "Epoch 24/30\n",
      "67/67 [==============================] - 24s 363ms/step - loss: 1.0152 - categorical_accuracy: 0.5771 - val_loss: 0.8497 - val_categorical_accuracy: 0.6300\n",
      "\n",
      "Epoch 00024: saving model to model_init_2020-12-2104_40_49.443651/model-00024-1.01523-0.57711-0.84966-0.63000.h5\n",
      "Epoch 25/30\n",
      "67/67 [==============================] - 23s 344ms/step - loss: 0.9172 - categorical_accuracy: 0.5970 - val_loss: 0.8429 - val_categorical_accuracy: 0.6500\n",
      "\n",
      "Epoch 00025: saving model to model_init_2020-12-2104_40_49.443651/model-00025-0.91720-0.59701-0.84290-0.65000.h5\n",
      "Epoch 26/30\n",
      "67/67 [==============================] - 23s 347ms/step - loss: 0.9021 - categorical_accuracy: 0.6418 - val_loss: 0.8415 - val_categorical_accuracy: 0.6100\n",
      "\n",
      "Epoch 00026: saving model to model_init_2020-12-2104_40_49.443651/model-00026-0.90207-0.64179-0.84150-0.61000.h5\n",
      "Epoch 27/30\n",
      "67/67 [==============================] - 25s 380ms/step - loss: 1.0017 - categorical_accuracy: 0.5522 - val_loss: 0.8261 - val_categorical_accuracy: 0.6600\n",
      "\n",
      "Epoch 00027: saving model to model_init_2020-12-2104_40_49.443651/model-00027-1.00166-0.55224-0.82608-0.66000.h5\n",
      "Epoch 28/30\n",
      "67/67 [==============================] - 25s 366ms/step - loss: 0.9306 - categorical_accuracy: 0.5721 - val_loss: 0.8744 - val_categorical_accuracy: 0.6100\n",
      "\n",
      "Epoch 00028: saving model to model_init_2020-12-2104_40_49.443651/model-00028-0.93062-0.57214-0.87436-0.61000.h5\n",
      "Epoch 29/30\n",
      "67/67 [==============================] - 23s 351ms/step - loss: 0.9189 - categorical_accuracy: 0.6517 - val_loss: 0.8239 - val_categorical_accuracy: 0.6700\n",
      "\n",
      "Epoch 00029: saving model to model_init_2020-12-2104_40_49.443651/model-00029-0.91886-0.65174-0.82386-0.67000.h5\n",
      "Epoch 30/30\n",
      "67/67 [==============================] - 24s 362ms/step - loss: 0.8717 - categorical_accuracy: 0.6119 - val_loss: 0.7713 - val_categorical_accuracy: 0.6800\n",
      "\n",
      "Epoch 00030: saving model to model_init_2020-12-2104_40_49.443651/model-00030-0.87166-0.61194-0.77133-0.68000.h5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fe19cd191d0>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 10\n",
    "train_generator = new_generator(train_path, train_doc, batch_size)\n",
    "val_generator = new_generator(val_path, val_doc, batch_size)\n",
    "num_epochs = 30\n",
    "model.fit_generator(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1, \n",
    "                    callbacks=callbacks_list, validation_data=val_generator, \n",
    "                    validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The acuuracy does not change much with the changing of batch size and epochs. It is stillhaving 68% validation accuracy. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Defining a random function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_random_affine():\n",
    "    dx, dy = np.random.randint(-1.7, 1.8, 2)\n",
    "    M = np.float32([[1, 0, dx], [0, 1, dy]])\n",
    "    return M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### New Generator with data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aug_generator(source_path, folder_list, batch_size):\n",
    "      \n",
    "    print( 'Source path = ', source_path, '; batch size =', batch_size)\n",
    "    img_idx = [x for x in range(0,nb_frames)] #create a list of image numbers you want to use for a particular video\n",
    "    while True:\n",
    "        t = np.random.permutation(folder_list)\n",
    "        num_batches = len(folder_list)//batch_size # calculate the number of batches\n",
    "        for batch in range(num_batches): # we iterate over the number of batches\n",
    "            batch_data = np.zeros((batch_size,nb_frames,nb_rows,nb_cols,nb_channel)) # x is the number of images you use for each video, (y,z) is the final size of the input images and 3 is the number of channels RGB\n",
    "            batch_labels = np.zeros((batch_size,5)) # batch_labels is the one hot representation of the output\n",
    "            \n",
    "            batch_data_aug = np.zeros((batch_size,nb_frames,nb_rows,nb_cols,nb_channel)) # x is the number of images you use for each video, (y,z) is the final size of the input images and 3 is the number of channels RGB\n",
    "            batch_labels_aug = np.zeros((batch_size,5)) # batch_labels is the one hot representation of the output\n",
    "  \n",
    "            batch_data_aug2 = np.zeros((batch_size,nb_frames,nb_rows,nb_cols,nb_channel)) # x is the number of images you use for each video, (y,z) is the final size of the input images and 3 is the number of channels RGB\n",
    "            batch_labels_aug2 = np.zeros((batch_size,5)) # batch_labels is the one hot representation of the output\n",
    " \n",
    "            batch_data_aug3 = np.zeros((batch_size,nb_frames,nb_rows,nb_cols,nb_channel)) # x is the number of images you use for each video, (y,z) is the final size of the input images and 3 is the number of channels RGB\n",
    "            batch_labels_aug3 = np.zeros((batch_size,5)) # batch_labels is the one hot representation of the output\n",
    "      \n",
    "            for folder in range(batch_size): # iterate over the batch_size\n",
    "                imgs = os.listdir(source_path+'/'+ t[folder + (batch*batch_size)].split(';')[0]) # read all the images in the folder\n",
    "                M = get_random_affine()\n",
    "                M2 = get_random_affine()\n",
    "                M3 = get_random_affine()\n",
    "                for idx,item in enumerate(img_idx): #  Iterate iver the frames/images of a folder to read them in\n",
    "                    image = cv2.imread(source_path+'/'+ t[folder + (batch*batch_size)].strip().split(';')[0]+'/'+imgs[item], cv2.IMREAD_COLOR)\n",
    "                    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "                    #crop the images and resize them. Note that the images are of 2 different shape \n",
    "                    #and the conv3D will throw error if the inputs in a batch have different shapes   \n",
    "                    resized = cv2.resize(image, (nb_rows,nb_cols), interpolation = cv2.INTER_AREA)\n",
    "                    batch_data[folder,idx] = resized\n",
    "                    batch_data_aug[folder,idx] = cv2.warpAffine(resized, M, (resized.shape[0], resized.shape[1]))\n",
    "                    batch_data_aug2[folder,idx] = cv2.warpAffine(resized, M2, (resized.shape[0], resized.shape[1]))\n",
    "                    batch_data_aug3[folder,idx] = cv2.warpAffine(resized, M3, (resized.shape[0], resized.shape[1]))\n",
    "                    \n",
    "                batch_labels[folder, int(t[folder + (batch*batch_size)].strip().split(';')[2])] = 1\n",
    "                batch_labels_aug[folder, int(t[folder + (batch*batch_size)].strip().split(';')[2])] = 1\n",
    "                batch_labels_aug2[folder, int(t[folder + (batch*batch_size)].strip().split(';')[2])] = 1\n",
    "                batch_labels_aug3[folder, int(t[folder + (batch*batch_size)].strip().split(';')[2])] = 1\n",
    "            \n",
    "            batch_data = np.append(batch_data, batch_data_aug, axis = 0) \n",
    "            batch_data = np.append(batch_data, batch_data_aug2, axis = 0) \n",
    "            batch_data = np.append(batch_data, batch_data_aug3, axis = 0)\n",
    "            batch_labels = np.append(batch_labels, batch_labels_aug, axis = 0) \n",
    "            batch_labels = np.append(batch_labels, batch_labels_aug2, axis = 0) \n",
    "            batch_labels = np.append(batch_labels, batch_labels_aug3, axis = 0)\n",
    "            yield batch_data, batch_labels #you yield the batch_data and the batch_labels, remember what does yield do\n",
    "\n",
    "        \n",
    "        # write the code for the remaining data points which are left after full batches\n",
    "        if (len(folder_list) != batch_size*num_batches):\n",
    "            print(\"Batch: \",num_batches+1,\"Index:\", batch_size)\n",
    "            batch_size = len(folder_list) - (batch_size*num_batches)\n",
    "            \n",
    "            batch_data = np.zeros((batch_size,nb_frames,nb_rows,nb_cols,nb_channel)) # x is the number of images you use for each video, (y,z) is the final size of the input images and 3 is the number of channels RGB\n",
    "            batch_labels = np.zeros((batch_size,5)) # batch_labels is the one hot representation of the output\n",
    "            \n",
    "            batch_data_aug = np.zeros((batch_size,nb_frames,nb_rows,nb_cols,nb_channel)) # x is the number of images you use for each video, (y,z) is the final size of the input images and 3 is the number of channels RGB\n",
    "            batch_labels_aug = np.zeros((batch_size,5)) # batch_labels is the one hot representation of the output\n",
    "  \n",
    "            batch_data_aug2 = np.zeros((batch_size,nb_frames,nb_rows,nb_cols,nb_channel)) # x is the number of images you use for each video, (y,z) is the final size of the input images and 3 is the number of channels RGB\n",
    "            batch_labels_aug2 = np.zeros((batch_size,5)) # batch_labels is the one hot representation of the output\n",
    "   \n",
    "            batch_data_aug3 = np.zeros((batch_size,nb_frames,nb_rows,nb_cols,nb_channel)) # x is the number of images you use for each video, (y,z) is the final size of the input images and 3 is the number of channels RGB\n",
    "            batch_labels_aug3 = np.zeros((batch_size,5)) # batch_labels is the one hot representation of the output\n",
    "      \n",
    "            for folder in range(batch_size): # iterate over the batch_size\n",
    "                imgs = os.listdir(source_path+'/'+ t[folder + (batch*batch_size)].split(';')[0]) # read all the images in the folder\n",
    "                M = get_random_affine()\n",
    "                M2 = get_random_affine()\n",
    "                M3 = get_random_affine()\n",
    "                for idx,item in enumerate(img_idx): #  Iterate iver the frames/images of a folder to read them in\n",
    "                    image = cv2.imread(source_path+'/'+ t[folder + (batch*batch_size)].strip().split(';')[0]+'/'+imgs[item], cv2.IMREAD_COLOR)\n",
    "                    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "                    #crop the images and resize them. Note that the images are of 2 different shape \n",
    "                    #and the conv3D will throw error if the inputs in a batch have different shapes\n",
    "                    resized = cv2.resize(image, (nb_rows,nb_cols), interpolation = cv2.INTER_AREA)\n",
    "                    batch_data[folder,idx] = resized\n",
    "                    batch_data_aug[folder,idx] = cv2.warpAffine(resized, M, (resized.shape[0], resized.shape[1]))\n",
    "                    batch_data_aug2[folder,idx] = cv2.warpAffine(resized, M2, (resized.shape[0], resized.shape[1]))\n",
    "                    batch_data_aug3[folder,idx] = cv2.warpAffine(resized, M3, (resized.shape[0], resized.shape[1]))\n",
    "                    \n",
    "                batch_labels[folder, int(t[folder + (batch*batch_size)].strip().split(';')[2])] = 1\n",
    "                batch_labels_aug[folder, int(t[folder + (batch*batch_size)].strip().split(';')[2])] = 1\n",
    "                batch_labels_aug2[folder, int(t[folder + (batch*batch_size)].strip().split(';')[2])] = 1\n",
    "                batch_labels_aug3[folder, int(t[folder + (batch*batch_size)].strip().split(';')[2])] = 1\n",
    "            \n",
    "            batch_data = np.append(batch_data, batch_data_aug, axis = 0) \n",
    "            batch_data = np.append(batch_data, batch_data_aug2, axis = 0) \n",
    "            batch_data = np.append(batch_data, batch_data_aug3, axis = 0)\n",
    "            batch_labels = np.append(batch_labels, batch_labels_aug, axis = 0) \n",
    "            batch_labels = np.append(batch_labels, batch_labels_aug2, axis = 0) \n",
    "            batch_labels = np.append(batch_labels, batch_labels_aug3, axis = 0) \n",
    "            yield batch_data, batch_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 7: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv3d_13 (Conv3D)           (None, 30, 120, 120, 8)   656       \n",
      "_________________________________________________________________\n",
      "batch_normalization_13 (Batc (None, 30, 120, 120, 8)   32        \n",
      "_________________________________________________________________\n",
      "activation_13 (Activation)   (None, 30, 120, 120, 8)   0         \n",
      "_________________________________________________________________\n",
      "max_pooling3d_13 (MaxPooling (None, 15, 60, 60, 8)     0         \n",
      "_________________________________________________________________\n",
      "conv3d_14 (Conv3D)           (None, 15, 60, 60, 16)    3472      \n",
      "_________________________________________________________________\n",
      "batch_normalization_14 (Batc (None, 15, 60, 60, 16)    64        \n",
      "_________________________________________________________________\n",
      "activation_14 (Activation)   (None, 15, 60, 60, 16)    0         \n",
      "_________________________________________________________________\n",
      "max_pooling3d_14 (MaxPooling (None, 7, 30, 30, 16)     0         \n",
      "_________________________________________________________________\n",
      "conv3d_15 (Conv3D)           (None, 7, 30, 30, 32)     4640      \n",
      "_________________________________________________________________\n",
      "batch_normalization_15 (Batc (None, 7, 30, 30, 32)     128       \n",
      "_________________________________________________________________\n",
      "activation_15 (Activation)   (None, 7, 30, 30, 32)     0         \n",
      "_________________________________________________________________\n",
      "max_pooling3d_15 (MaxPooling (None, 3, 15, 15, 32)     0         \n",
      "_________________________________________________________________\n",
      "conv3d_16 (Conv3D)           (None, 3, 15, 15, 64)     18496     \n",
      "_________________________________________________________________\n",
      "batch_normalization_16 (Batc (None, 3, 15, 15, 64)     256       \n",
      "_________________________________________________________________\n",
      "activation_16 (Activation)   (None, 3, 15, 15, 64)     0         \n",
      "_________________________________________________________________\n",
      "max_pooling3d_16 (MaxPooling (None, 1, 7, 7, 64)       0         \n",
      "_________________________________________________________________\n",
      "flatten_4 (Flatten)          (None, 3136)              0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 1000)              3137000   \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 1000)              0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 500)               500500    \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 500)               0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 5)                 2505      \n",
      "=================================================================\n",
      "Total params: 3,667,749\n",
      "Trainable params: 3,667,509\n",
      "Non-trainable params: 240\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "nb_frames = 30 # number of frames\n",
    "nb_rows = 120 # image width\n",
    "nb_cols = 120 # image height \n",
    "\n",
    "nb_classes = 5\n",
    "nb_channel = 3\n",
    "\n",
    "input_shape=(nb_frames,nb_rows,nb_cols,nb_channel)\n",
    "\n",
    "# Define model\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Conv3D(nb_filters[0], \n",
    "                 kernel_size=(3,3,3), \n",
    "                 input_shape=input_shape,\n",
    "                 padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(MaxPooling3D(pool_size=(2,2,2)))\n",
    "\n",
    "model.add(Conv3D(nb_filters[1], \n",
    "                 kernel_size=(3,3,3), \n",
    "                 padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(MaxPooling3D(pool_size=(2,2,2)))\n",
    "\n",
    "model.add(Conv3D(nb_filters[2], \n",
    "                 kernel_size=(1,3,3), \n",
    "                 padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(MaxPooling3D(pool_size=(2,2,2)))\n",
    "\n",
    "model.add(Conv3D(nb_filters[3], \n",
    "                 kernel_size=(1,3,3), \n",
    "                 padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(MaxPooling3D(pool_size=(2,2,2)))\n",
    "\n",
    "#Flatten Layers\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(nb_dense[0], activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(nb_dense[1], activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "#softmax layer\n",
    "model.add(Dense(nb_dense[2], activation='softmax'))\n",
    "model.compile(optimizer=Adam(), loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source path =  ./Project_data/val ; batch size = 5\n",
      "Source path =  ./Project_data/train ; batch size = 5\n",
      "Epoch 1/20\n",
      "67/67 [==============================] - 64s 963ms/step - loss: 4.4215 - categorical_accuracy: 0.2769 - val_loss: 1.8285 - val_categorical_accuracy: 0.3400\n",
      "\n",
      "Epoch 00001: saving model to model_init_2020-12-2104_40_49.443651/model-00001-4.42153-0.27687-1.82849-0.34000.h5\n",
      "Epoch 2/20\n",
      "64/67 [===========================>..] - ETA: 2s - loss: 1.4069 - categorical_accuracy: 0.4164Batch:  133 Index: 5\n",
      "67/67 [==============================] - 59s 886ms/step - loss: 1.3948 - categorical_accuracy: 0.4228 - val_loss: 1.5687 - val_categorical_accuracy: 0.3150\n",
      "\n",
      "Epoch 00002: saving model to model_init_2020-12-2104_40_49.443651/model-00002-1.39836-0.41994-1.56870-0.31500.h5\n",
      "Epoch 3/20\n",
      "67/67 [==============================] - 34s 505ms/step - loss: 1.3275 - categorical_accuracy: 0.4440 - val_loss: 1.0777 - val_categorical_accuracy: 0.5600\n",
      "\n",
      "Epoch 00003: saving model to model_init_2020-12-2104_40_49.443651/model-00003-1.32751-0.44403-1.07774-0.56000.h5\n",
      "Epoch 4/20\n",
      "67/67 [==============================] - 35s 518ms/step - loss: 1.3881 - categorical_accuracy: 0.4366 - val_loss: 1.1841 - val_categorical_accuracy: 0.4600\n",
      "\n",
      "Epoch 00004: saving model to model_init_2020-12-2104_40_49.443651/model-00004-1.38808-0.43657-1.18408-0.46000.h5\n",
      "Epoch 5/20\n",
      "67/67 [==============================] - 34s 512ms/step - loss: 1.3258 - categorical_accuracy: 0.4490 - val_loss: 1.4816 - val_categorical_accuracy: 0.3600\n",
      "\n",
      "Epoch 00005: saving model to model_init_2020-12-2104_40_49.443651/model-00005-1.32577-0.44900-1.48157-0.36000.h5\n",
      "\n",
      "Epoch 00005: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "Epoch 6/20\n",
      "67/67 [==============================] - 35s 518ms/step - loss: 1.2218 - categorical_accuracy: 0.4900 - val_loss: 1.0344 - val_categorical_accuracy: 0.5500\n",
      "\n",
      "Epoch 00006: saving model to model_init_2020-12-2104_40_49.443651/model-00006-1.22179-0.49005-1.03436-0.55000.h5\n",
      "Epoch 7/20\n",
      "67/67 [==============================] - 35s 523ms/step - loss: 1.0706 - categorical_accuracy: 0.5485 - val_loss: 0.8951 - val_categorical_accuracy: 0.6850\n",
      "\n",
      "Epoch 00007: saving model to model_init_2020-12-2104_40_49.443651/model-00007-1.07056-0.54851-0.89511-0.68500.h5\n",
      "Epoch 8/20\n",
      "67/67 [==============================] - 34s 512ms/step - loss: 1.0417 - categorical_accuracy: 0.5746 - val_loss: 0.9119 - val_categorical_accuracy: 0.6200\n",
      "\n",
      "Epoch 00008: saving model to model_init_2020-12-2104_40_49.443651/model-00008-1.04174-0.57463-0.91186-0.62000.h5\n",
      "Epoch 9/20\n",
      "67/67 [==============================] - 36s 530ms/step - loss: 0.9399 - categorical_accuracy: 0.6082 - val_loss: 0.8332 - val_categorical_accuracy: 0.6500\n",
      "\n",
      "Epoch 00009: saving model to model_init_2020-12-2104_40_49.443651/model-00009-0.93991-0.60821-0.83320-0.65000.h5\n",
      "Epoch 10/20\n",
      "67/67 [==============================] - 35s 521ms/step - loss: 0.8683 - categorical_accuracy: 0.6617 - val_loss: 0.7777 - val_categorical_accuracy: 0.7000\n",
      "\n",
      "Epoch 00010: saving model to model_init_2020-12-2104_40_49.443651/model-00010-0.86830-0.66169-0.77766-0.70000.h5\n",
      "Epoch 11/20\n",
      "67/67 [==============================] - 36s 534ms/step - loss: 0.9240 - categorical_accuracy: 0.6517 - val_loss: 0.8712 - val_categorical_accuracy: 0.5950\n",
      "\n",
      "Epoch 00011: saving model to model_init_2020-12-2104_40_49.443651/model-00011-0.92400-0.65174-0.87116-0.59500.h5\n",
      "Epoch 12/20\n",
      "67/67 [==============================] - 36s 532ms/step - loss: 0.8801 - categorical_accuracy: 0.6430 - val_loss: 0.9324 - val_categorical_accuracy: 0.7300\n",
      "\n",
      "Epoch 00012: saving model to model_init_2020-12-2104_40_49.443651/model-00012-0.88014-0.64303-0.93244-0.73000.h5\n",
      "\n",
      "Epoch 00012: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "Epoch 13/20\n",
      "67/67 [==============================] - 34s 512ms/step - loss: 0.6865 - categorical_accuracy: 0.7376 - val_loss: 0.7680 - val_categorical_accuracy: 0.6700\n",
      "\n",
      "Epoch 00013: saving model to model_init_2020-12-2104_40_49.443651/model-00013-0.68648-0.73756-0.76800-0.67000.h5\n",
      "Epoch 14/20\n",
      "67/67 [==============================] - 35s 528ms/step - loss: 0.6849 - categorical_accuracy: 0.7413 - val_loss: 0.5586 - val_categorical_accuracy: 0.7900\n",
      "\n",
      "Epoch 00014: saving model to model_init_2020-12-2104_40_49.443651/model-00014-0.68495-0.74129-0.55864-0.79000.h5\n",
      "Epoch 15/20\n",
      "67/67 [==============================] - 34s 512ms/step - loss: 0.6527 - categorical_accuracy: 0.7687 - val_loss: 0.6409 - val_categorical_accuracy: 0.7000\n",
      "\n",
      "Epoch 00015: saving model to model_init_2020-12-2104_40_49.443651/model-00015-0.65267-0.76866-0.64089-0.70000.h5\n",
      "Epoch 16/20\n",
      "67/67 [==============================] - 34s 511ms/step - loss: 0.5864 - categorical_accuracy: 0.7749 - val_loss: 0.5294 - val_categorical_accuracy: 0.7850\n",
      "\n",
      "Epoch 00016: saving model to model_init_2020-12-2104_40_49.443651/model-00016-0.58636-0.77488-0.52938-0.78500.h5\n",
      "Epoch 17/20\n",
      "67/67 [==============================] - 34s 502ms/step - loss: 0.5962 - categorical_accuracy: 0.7649 - val_loss: 0.6801 - val_categorical_accuracy: 0.7250\n",
      "\n",
      "Epoch 00017: saving model to model_init_2020-12-2104_40_49.443651/model-00017-0.59616-0.76493-0.68014-0.72500.h5\n",
      "Epoch 18/20\n",
      "67/67 [==============================] - 34s 514ms/step - loss: 0.5156 - categorical_accuracy: 0.8085 - val_loss: 0.5840 - val_categorical_accuracy: 0.8400\n",
      "\n",
      "Epoch 00018: saving model to model_init_2020-12-2104_40_49.443651/model-00018-0.51557-0.80846-0.58396-0.84000.h5\n",
      "\n",
      "Epoch 00018: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "Epoch 19/20\n",
      "67/67 [==============================] - 33s 496ms/step - loss: 0.4949 - categorical_accuracy: 0.8172 - val_loss: 0.7620 - val_categorical_accuracy: 0.7400\n",
      "\n",
      "Epoch 00019: saving model to model_init_2020-12-2104_40_49.443651/model-00019-0.49493-0.81716-0.76203-0.74000.h5\n",
      "Epoch 20/20\n",
      "67/67 [==============================] - 34s 511ms/step - loss: 0.4935 - categorical_accuracy: 0.7985 - val_loss: 0.6010 - val_categorical_accuracy: 0.7500\n",
      "\n",
      "Epoch 00020: saving model to model_init_2020-12-2104_40_49.443651/model-00020-0.49350-0.79851-0.60103-0.75000.h5\n",
      "\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fe1b0334630>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 5\n",
    "train_generator = aug_generator(train_path, train_doc, batch_size)\n",
    "val_generator = aug_generator(val_path, val_doc, batch_size)\n",
    "num_epochs = 20\n",
    "model.fit_generator(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1, \n",
    "                    callbacks=callbacks_list, validation_data=val_generator, \n",
    "                    validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### It has a accuray of 84%. \n",
    "\n",
    "### Model 8: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv3d_17 (Conv3D)           (None, 30, 120, 120, 8)   656       \n",
      "_________________________________________________________________\n",
      "batch_normalization_17 (Batc (None, 30, 120, 120, 8)   32        \n",
      "_________________________________________________________________\n",
      "activation_17 (Activation)   (None, 30, 120, 120, 8)   0         \n",
      "_________________________________________________________________\n",
      "max_pooling3d_17 (MaxPooling (None, 15, 60, 60, 8)     0         \n",
      "_________________________________________________________________\n",
      "conv3d_18 (Conv3D)           (None, 15, 60, 60, 16)    3472      \n",
      "_________________________________________________________________\n",
      "batch_normalization_18 (Batc (None, 15, 60, 60, 16)    64        \n",
      "_________________________________________________________________\n",
      "activation_18 (Activation)   (None, 15, 60, 60, 16)    0         \n",
      "_________________________________________________________________\n",
      "max_pooling3d_18 (MaxPooling (None, 7, 30, 30, 16)     0         \n",
      "_________________________________________________________________\n",
      "conv3d_19 (Conv3D)           (None, 7, 30, 30, 32)     4640      \n",
      "_________________________________________________________________\n",
      "batch_normalization_19 (Batc (None, 7, 30, 30, 32)     128       \n",
      "_________________________________________________________________\n",
      "activation_19 (Activation)   (None, 7, 30, 30, 32)     0         \n",
      "_________________________________________________________________\n",
      "max_pooling3d_19 (MaxPooling (None, 3, 15, 15, 32)     0         \n",
      "_________________________________________________________________\n",
      "conv3d_20 (Conv3D)           (None, 3, 15, 15, 64)     18496     \n",
      "_________________________________________________________________\n",
      "batch_normalization_20 (Batc (None, 3, 15, 15, 64)     256       \n",
      "_________________________________________________________________\n",
      "activation_20 (Activation)   (None, 3, 15, 15, 64)     0         \n",
      "_________________________________________________________________\n",
      "max_pooling3d_20 (MaxPooling (None, 1, 7, 7, 64)       0         \n",
      "_________________________________________________________________\n",
      "flatten_5 (Flatten)          (None, 3136)              0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 1000)              3137000   \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 1000)              0         \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 500)               500500    \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 500)               0         \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 5)                 2505      \n",
      "=================================================================\n",
      "Total params: 3,667,749\n",
      "Trainable params: 3,667,509\n",
      "Non-trainable params: 240\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "nb_frames = 30 # number of frames\n",
    "nb_rows = 120 # image width\n",
    "nb_cols = 120 # image height \n",
    "\n",
    "nb_classes = 5\n",
    "nb_channel = 3\n",
    "\n",
    "input_shape=(nb_frames,nb_rows,nb_cols,nb_channel)\n",
    "\n",
    "# Define model\n",
    "model1a = Sequential()\n",
    "\n",
    "model1a.add(Conv3D(nb_filters[0], \n",
    "                 kernel_size=(3,3,3), \n",
    "                 input_shape=input_shape,\n",
    "                 padding='same'))\n",
    "model1a.add(BatchNormalization())\n",
    "model1a.add(Activation('relu'))\n",
    "\n",
    "model1a.add(MaxPooling3D(pool_size=(2,2,2)))\n",
    "\n",
    "model1a.add(Conv3D(nb_filters[1], \n",
    "                 kernel_size=(3,3,3), \n",
    "                 padding='same'))\n",
    "model1a.add(BatchNormalization())\n",
    "model1a.add(Activation('relu'))\n",
    "\n",
    "model1a.add(MaxPooling3D(pool_size=(2,2,2)))\n",
    "\n",
    "model1a.add(Conv3D(nb_filters[2], \n",
    "                 kernel_size=(1,3,3), \n",
    "                 padding='same'))\n",
    "model1a.add(BatchNormalization())\n",
    "model1a.add(Activation('relu'))\n",
    "\n",
    "model1a.add(MaxPooling3D(pool_size=(2,2,2)))\n",
    "\n",
    "model1a.add(Conv3D(nb_filters[3], \n",
    "                 kernel_size=(1,3,3), \n",
    "                 padding='same'))\n",
    "model1a.add(BatchNormalization())\n",
    "model1a.add(Activation('relu'))\n",
    "\n",
    "model1a.add(MaxPooling3D(pool_size=(2,2,2)))\n",
    "\n",
    "#Flatten Layers\n",
    "model1a.add(Flatten())\n",
    "\n",
    "model1a.add(Dense(nb_dense[0], activation='relu'))\n",
    "model1a.add(Dropout(0.5))\n",
    "\n",
    "model1a.add(Dense(nb_dense[1], activation='relu'))\n",
    "model1a.add(Dropout(0.5))\n",
    "\n",
    "#softmax layer\n",
    "model1a.add(Dense(nb_dense[2], activation='softmax'))\n",
    "model1a.compile(optimizer=Adam(), loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "model1a.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source path =  ./Project_data/val ; batch size = 10\n",
      "Source path =  ./Project_data/train ; batch size = 10\n",
      "Epoch 1/20\n",
      "65/67 [============================>.] - ETA: 3s - loss: 2.6908 - categorical_accuracy: 0.3165Batch:  67 Index: 10\n",
      "67/67 [==============================] - 124s 2s/step - loss: 2.6555 - categorical_accuracy: 0.3173 - val_loss: 1.4686 - val_categorical_accuracy: 0.3225\n",
      "\n",
      "Epoch 00001: saving model to model_init_2020-12-2104_40_49.443651/model-00001-2.66845-0.31712-1.46856-0.32250.h5\n",
      "Epoch 2/20\n",
      "67/67 [==============================] - 38s 568ms/step - loss: 1.6111 - categorical_accuracy: 0.3632 - val_loss: 1.2953 - val_categorical_accuracy: 0.4725\n",
      "\n",
      "Epoch 00002: saving model to model_init_2020-12-2104_40_49.443651/model-00002-1.61110-0.36318-1.29532-0.47250.h5\n",
      "Epoch 3/20\n",
      "67/67 [==============================] - 37s 556ms/step - loss: 1.5472 - categorical_accuracy: 0.3383 - val_loss: 1.4417 - val_categorical_accuracy: 0.3000\n",
      "\n",
      "Epoch 00003: saving model to model_init_2020-12-2104_40_49.443651/model-00003-1.54718-0.33831-1.44166-0.30000.h5\n",
      "Epoch 4/20\n",
      "67/67 [==============================] - 37s 555ms/step - loss: 1.5352 - categorical_accuracy: 0.3134 - val_loss: 1.3836 - val_categorical_accuracy: 0.3950\n",
      "\n",
      "Epoch 00004: saving model to model_init_2020-12-2104_40_49.443651/model-00004-1.53517-0.31343-1.38357-0.39500.h5\n",
      "\n",
      "Epoch 00004: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "Epoch 5/20\n",
      "67/67 [==============================] - 37s 552ms/step - loss: 1.2937 - categorical_accuracy: 0.4478 - val_loss: 1.2024 - val_categorical_accuracy: 0.4575\n",
      "\n",
      "Epoch 00005: saving model to model_init_2020-12-2104_40_49.443651/model-00005-1.29366-0.44776-1.20238-0.45750.h5\n",
      "Epoch 6/20\n",
      "67/67 [==============================] - 37s 556ms/step - loss: 1.3092 - categorical_accuracy: 0.4353 - val_loss: 1.2778 - val_categorical_accuracy: 0.4575\n",
      "\n",
      "Epoch 00006: saving model to model_init_2020-12-2104_40_49.443651/model-00006-1.30920-0.43532-1.27780-0.45750.h5\n",
      "Epoch 7/20\n",
      "67/67 [==============================] - 37s 552ms/step - loss: 1.1837 - categorical_accuracy: 0.4764 - val_loss: 1.1520 - val_categorical_accuracy: 0.4400\n",
      "\n",
      "Epoch 00007: saving model to model_init_2020-12-2104_40_49.443651/model-00007-1.18367-0.47637-1.15199-0.44000.h5\n",
      "Epoch 8/20\n",
      "67/67 [==============================] - 37s 549ms/step - loss: 1.3004 - categorical_accuracy: 0.4664 - val_loss: 1.0791 - val_categorical_accuracy: 0.5550\n",
      "\n",
      "Epoch 00008: saving model to model_init_2020-12-2104_40_49.443651/model-00008-1.30039-0.46642-1.07908-0.55500.h5\n",
      "Epoch 9/20\n",
      "67/67 [==============================] - 37s 548ms/step - loss: 1.0590 - categorical_accuracy: 0.5510 - val_loss: 0.9880 - val_categorical_accuracy: 0.6225\n",
      "\n",
      "Epoch 00009: saving model to model_init_2020-12-2104_40_49.443651/model-00009-1.05899-0.55100-0.98797-0.62250.h5\n",
      "Epoch 10/20\n",
      "67/67 [==============================] - 37s 548ms/step - loss: 1.1554 - categorical_accuracy: 0.5100 - val_loss: 1.0597 - val_categorical_accuracy: 0.6175\n",
      "\n",
      "Epoch 00010: saving model to model_init_2020-12-2104_40_49.443651/model-00010-1.15537-0.50995-1.05975-0.61750.h5\n",
      "Epoch 11/20\n",
      "67/67 [==============================] - 37s 546ms/step - loss: 1.0707 - categorical_accuracy: 0.5771 - val_loss: 0.9269 - val_categorical_accuracy: 0.6625\n",
      "\n",
      "Epoch 00011: saving model to model_init_2020-12-2104_40_49.443651/model-00011-1.07070-0.57711-0.92691-0.66250.h5\n",
      "Epoch 12/20\n",
      "67/67 [==============================] - 37s 550ms/step - loss: 1.1265 - categorical_accuracy: 0.5547 - val_loss: 0.9833 - val_categorical_accuracy: 0.6500\n",
      "\n",
      "Epoch 00012: saving model to model_init_2020-12-2104_40_49.443651/model-00012-1.12650-0.55473-0.98330-0.65000.h5\n",
      "Epoch 13/20\n",
      "67/67 [==============================] - 37s 547ms/step - loss: 0.8727 - categorical_accuracy: 0.6580 - val_loss: 0.8204 - val_categorical_accuracy: 0.6500\n",
      "\n",
      "Epoch 00013: saving model to model_init_2020-12-2104_40_49.443651/model-00013-0.87269-0.65796-0.82037-0.65000.h5\n",
      "Epoch 14/20\n",
      "67/67 [==============================] - 37s 548ms/step - loss: 1.0935 - categorical_accuracy: 0.5348 - val_loss: 0.8410 - val_categorical_accuracy: 0.6750\n",
      "\n",
      "Epoch 00014: saving model to model_init_2020-12-2104_40_49.443651/model-00014-1.09347-0.53483-0.84100-0.67500.h5\n",
      "Epoch 15/20\n",
      "67/67 [==============================] - 37s 546ms/step - loss: 0.9156 - categorical_accuracy: 0.6393 - val_loss: 1.0840 - val_categorical_accuracy: 0.5325\n",
      "\n",
      "Epoch 00015: saving model to model_init_2020-12-2104_40_49.443651/model-00015-0.91556-0.63930-1.08400-0.53250.h5\n",
      "\n",
      "Epoch 00015: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "Epoch 16/20\n",
      "67/67 [==============================] - 37s 554ms/step - loss: 0.8733 - categorical_accuracy: 0.6393 - val_loss: 0.7975 - val_categorical_accuracy: 0.6550\n",
      "\n",
      "Epoch 00016: saving model to model_init_2020-12-2104_40_49.443651/model-00016-0.87331-0.63930-0.79753-0.65500.h5\n",
      "Epoch 17/20\n",
      "67/67 [==============================] - 37s 558ms/step - loss: 0.9091 - categorical_accuracy: 0.6244 - val_loss: 0.9451 - val_categorical_accuracy: 0.5825\n",
      "\n",
      "Epoch 00017: saving model to model_init_2020-12-2104_40_49.443651/model-00017-0.90907-0.62438-0.94510-0.58250.h5\n",
      "Epoch 18/20\n",
      "67/67 [==============================] - 38s 565ms/step - loss: 0.6932 - categorical_accuracy: 0.7214 - val_loss: 0.7618 - val_categorical_accuracy: 0.6775\n",
      "\n",
      "Epoch 00018: saving model to model_init_2020-12-2104_40_49.443651/model-00018-0.69315-0.72139-0.76176-0.67750.h5\n",
      "Epoch 19/20\n",
      "67/67 [==============================] - 37s 555ms/step - loss: 0.7062 - categorical_accuracy: 0.7214 - val_loss: 0.7057 - val_categorical_accuracy: 0.7200\n",
      "\n",
      "Epoch 00019: saving model to model_init_2020-12-2104_40_49.443651/model-00019-0.70624-0.72139-0.70565-0.72000.h5\n",
      "Epoch 20/20\n",
      "67/67 [==============================] - 38s 563ms/step - loss: 0.6368 - categorical_accuracy: 0.7413 - val_loss: 0.6704 - val_categorical_accuracy: 0.7450\n",
      "\n",
      "Epoch 00020: saving model to model_init_2020-12-2104_40_49.443651/model-00020-0.63677-0.74129-0.67044-0.74500.h5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fe1b03c6e48>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 10\n",
    "train_generator = aug_generator(train_path, train_doc, batch_size)\n",
    "val_generator = aug_generator(val_path, val_doc, batch_size)\n",
    "num_epochs = 20\n",
    "model1a.fit_generator(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1, \n",
    "                    callbacks=callbacks_list, validation_data=val_generator, \n",
    "                    validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### It has a validation accuracy of 74.5 %\n",
    "\n",
    "##### Changing the Batch Size to 5 and Epochs to 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source path =  ./Project_data/val ; batch size = 5\n",
      "Source path =  ./Project_data/train ; batch size = 5\n",
      "Epoch 1/30\n",
      "67/67 [==============================] - 63s 942ms/step - loss: 0.5654 - categorical_accuracy: 0.7896 - val_loss: 0.6765 - val_categorical_accuracy: 0.7750\n",
      "\n",
      "Epoch 00001: saving model to model_init_2020-12-2104_40_49.443651/model-00001-0.56535-0.78955-0.67651-0.77500.h5\n",
      "Epoch 2/30\n",
      "64/67 [===========================>..] - ETA: 2s - loss: 0.6784 - categorical_accuracy: 0.7625Batch:  133 Index: 5\n",
      "67/67 [==============================] - 59s 874ms/step - loss: 0.6647 - categorical_accuracy: 0.7652 - val_loss: 0.5568 - val_categorical_accuracy: 0.7900\n",
      "\n",
      "Epoch 00002: saving model to model_init_2020-12-2104_40_49.443651/model-00002-0.66812-0.76435-0.55681-0.79000.h5\n",
      "Epoch 3/30\n",
      "67/67 [==============================] - 35s 526ms/step - loss: 0.6184 - categorical_accuracy: 0.7562 - val_loss: 0.8051 - val_categorical_accuracy: 0.6050\n",
      "\n",
      "Epoch 00003: saving model to model_init_2020-12-2104_40_49.443651/model-00003-0.61839-0.75622-0.80507-0.60500.h5\n",
      "Epoch 4/30\n",
      "67/67 [==============================] - 36s 536ms/step - loss: 0.5372 - categorical_accuracy: 0.7761 - val_loss: 0.7846 - val_categorical_accuracy: 0.7200\n",
      "\n",
      "Epoch 00004: saving model to model_init_2020-12-2104_40_49.443651/model-00004-0.53722-0.77612-0.78461-0.72000.h5\n",
      "\n",
      "Epoch 00004: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "Epoch 5/30\n",
      "67/67 [==============================] - 34s 511ms/step - loss: 0.6062 - categorical_accuracy: 0.7774 - val_loss: 0.6253 - val_categorical_accuracy: 0.7800\n",
      "\n",
      "Epoch 00005: saving model to model_init_2020-12-2104_40_49.443651/model-00005-0.60623-0.77736-0.62531-0.78000.h5\n",
      "Epoch 6/30\n",
      "67/67 [==============================] - 35s 527ms/step - loss: 0.6070 - categorical_accuracy: 0.7873 - val_loss: 0.5723 - val_categorical_accuracy: 0.7750\n",
      "\n",
      "Epoch 00006: saving model to model_init_2020-12-2104_40_49.443651/model-00006-0.60698-0.78731-0.57228-0.77500.h5\n",
      "\n",
      "Epoch 00006: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "Epoch 7/30\n",
      "67/67 [==============================] - 35s 521ms/step - loss: 0.4764 - categorical_accuracy: 0.8060 - val_loss: 0.5647 - val_categorical_accuracy: 0.8250\n",
      "\n",
      "Epoch 00007: saving model to model_init_2020-12-2104_40_49.443651/model-00007-0.47637-0.80597-0.56474-0.82500.h5\n",
      "Epoch 8/30\n",
      "67/67 [==============================] - 38s 564ms/step - loss: 0.4581 - categorical_accuracy: 0.8321 - val_loss: 0.5939 - val_categorical_accuracy: 0.8100\n",
      "\n",
      "Epoch 00008: saving model to model_init_2020-12-2104_40_49.443651/model-00008-0.45809-0.83209-0.59388-0.81000.h5\n",
      "\n",
      "Epoch 00008: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "Epoch 9/30\n",
      "67/67 [==============================] - 36s 532ms/step - loss: 0.3845 - categorical_accuracy: 0.8731 - val_loss: 0.6245 - val_categorical_accuracy: 0.7650\n",
      "\n",
      "Epoch 00009: saving model to model_init_2020-12-2104_40_49.443651/model-00009-0.38445-0.87313-0.62449-0.76500.h5\n",
      "Epoch 10/30\n",
      "67/67 [==============================] - 36s 531ms/step - loss: 0.3995 - categorical_accuracy: 0.8706 - val_loss: 0.4693 - val_categorical_accuracy: 0.8100\n",
      "\n",
      "Epoch 00010: saving model to model_init_2020-12-2104_40_49.443651/model-00010-0.39955-0.87065-0.46929-0.81000.h5\n",
      "Epoch 11/30\n",
      "67/67 [==============================] - 37s 549ms/step - loss: 0.4479 - categorical_accuracy: 0.8458 - val_loss: 0.4013 - val_categorical_accuracy: 0.8700\n",
      "\n",
      "Epoch 00011: saving model to model_init_2020-12-2104_40_49.443651/model-00011-0.44791-0.84577-0.40131-0.87000.h5\n",
      "Epoch 12/30\n",
      "67/67 [==============================] - 35s 523ms/step - loss: 0.3941 - categorical_accuracy: 0.8557 - val_loss: 0.6746 - val_categorical_accuracy: 0.7200\n",
      "\n",
      "Epoch 00012: saving model to model_init_2020-12-2104_40_49.443651/model-00012-0.39409-0.85572-0.67465-0.72000.h5\n",
      "Epoch 13/30\n",
      "67/67 [==============================] - 36s 537ms/step - loss: 0.4256 - categorical_accuracy: 0.8570 - val_loss: 0.4693 - val_categorical_accuracy: 0.8350\n",
      "\n",
      "Epoch 00013: saving model to model_init_2020-12-2104_40_49.443651/model-00013-0.42559-0.85697-0.46928-0.83500.h5\n",
      "\n",
      "Epoch 00013: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
      "Epoch 14/30\n",
      "67/67 [==============================] - 37s 555ms/step - loss: 0.3774 - categorical_accuracy: 0.8682 - val_loss: 0.6261 - val_categorical_accuracy: 0.7950\n",
      "\n",
      "Epoch 00014: saving model to model_init_2020-12-2104_40_49.443651/model-00014-0.37736-0.86816-0.62614-0.79500.h5\n",
      "Epoch 15/30\n",
      "67/67 [==============================] - 37s 546ms/step - loss: 0.4289 - categorical_accuracy: 0.8259 - val_loss: 0.5499 - val_categorical_accuracy: 0.7950\n",
      "\n",
      "Epoch 00015: saving model to model_init_2020-12-2104_40_49.443651/model-00015-0.42894-0.82587-0.54991-0.79500.h5\n",
      "\n",
      "Epoch 00015: ReduceLROnPlateau reducing learning rate to 7.812500371073838e-06.\n",
      "Epoch 16/30\n",
      "67/67 [==============================] - 36s 539ms/step - loss: 0.4170 - categorical_accuracy: 0.8495 - val_loss: 0.5043 - val_categorical_accuracy: 0.8250\n",
      "\n",
      "Epoch 00016: saving model to model_init_2020-12-2104_40_49.443651/model-00016-0.41702-0.84950-0.50431-0.82500.h5\n",
      "Epoch 17/30\n",
      "67/67 [==============================] - 36s 533ms/step - loss: 0.3700 - categorical_accuracy: 0.8719 - val_loss: 0.4684 - val_categorical_accuracy: 0.7950\n",
      "\n",
      "Epoch 00017: saving model to model_init_2020-12-2104_40_49.443651/model-00017-0.37002-0.87189-0.46840-0.79500.h5\n",
      "\n",
      "Epoch 00017: ReduceLROnPlateau reducing learning rate to 3.906250185536919e-06.\n",
      "Epoch 18/30\n",
      "67/67 [==============================] - 36s 535ms/step - loss: 0.4167 - categorical_accuracy: 0.8495 - val_loss: 0.5990 - val_categorical_accuracy: 0.8100\n",
      "\n",
      "Epoch 00018: saving model to model_init_2020-12-2104_40_49.443651/model-00018-0.41672-0.84950-0.59897-0.81000.h5\n",
      "Epoch 19/30\n",
      "67/67 [==============================] - 36s 537ms/step - loss: 0.4241 - categorical_accuracy: 0.8221 - val_loss: 0.5556 - val_categorical_accuracy: 0.7650\n",
      "\n",
      "Epoch 00019: saving model to model_init_2020-12-2104_40_49.443651/model-00019-0.42411-0.82214-0.55560-0.76500.h5\n",
      "\n",
      "Epoch 00019: ReduceLROnPlateau reducing learning rate to 1.9531250927684596e-06.\n",
      "Epoch 20/30\n",
      "67/67 [==============================] - 34s 508ms/step - loss: 0.3740 - categorical_accuracy: 0.8794 - val_loss: 0.5064 - val_categorical_accuracy: 0.8600\n",
      "\n",
      "Epoch 00020: saving model to model_init_2020-12-2104_40_49.443651/model-00020-0.37405-0.87935-0.50645-0.86000.h5\n",
      "Epoch 21/30\n",
      "67/67 [==============================] - 36s 530ms/step - loss: 0.3780 - categorical_accuracy: 0.8682 - val_loss: 0.6027 - val_categorical_accuracy: 0.7600\n",
      "\n",
      "Epoch 00021: saving model to model_init_2020-12-2104_40_49.443651/model-00021-0.37802-0.86816-0.60271-0.76000.h5\n",
      "\n",
      "Epoch 00021: ReduceLROnPlateau reducing learning rate to 9.765625463842298e-07.\n",
      "Epoch 22/30\n",
      "67/67 [==============================] - 36s 536ms/step - loss: 0.3765 - categorical_accuracy: 0.8632 - val_loss: 0.4639 - val_categorical_accuracy: 0.8500\n",
      "\n",
      "Epoch 00022: saving model to model_init_2020-12-2104_40_49.443651/model-00022-0.37646-0.86318-0.46394-0.85000.h5\n",
      "Epoch 23/30\n",
      "67/67 [==============================] - 36s 536ms/step - loss: 0.4205 - categorical_accuracy: 0.8644 - val_loss: 0.6011 - val_categorical_accuracy: 0.7600\n",
      "\n",
      "Epoch 00023: saving model to model_init_2020-12-2104_40_49.443651/model-00023-0.42054-0.86443-0.60109-0.76000.h5\n",
      "\n",
      "Epoch 00023: ReduceLROnPlateau reducing learning rate to 4.882812731921149e-07.\n",
      "Epoch 24/30\n",
      "67/67 [==============================] - 35s 521ms/step - loss: 0.3579 - categorical_accuracy: 0.8868 - val_loss: 0.4698 - val_categorical_accuracy: 0.8550\n",
      "\n",
      "Epoch 00024: saving model to model_init_2020-12-2104_40_49.443651/model-00024-0.35786-0.88682-0.46976-0.85500.h5\n",
      "Epoch 25/30\n",
      "67/67 [==============================] - 36s 535ms/step - loss: 0.3997 - categorical_accuracy: 0.8507 - val_loss: 0.5754 - val_categorical_accuracy: 0.7750\n",
      "\n",
      "Epoch 00025: saving model to model_init_2020-12-2104_40_49.443651/model-00025-0.39968-0.85075-0.57542-0.77500.h5\n",
      "\n",
      "Epoch 00025: ReduceLROnPlateau reducing learning rate to 2.4414063659605745e-07.\n",
      "Epoch 26/30\n",
      "67/67 [==============================] - 35s 528ms/step - loss: 0.3545 - categorical_accuracy: 0.8794 - val_loss: 0.4877 - val_categorical_accuracy: 0.8350\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00026: saving model to model_init_2020-12-2104_40_49.443651/model-00026-0.35451-0.87935-0.48768-0.83500.h5\n",
      "Epoch 27/30\n",
      "67/67 [==============================] - 34s 514ms/step - loss: 0.4422 - categorical_accuracy: 0.8358 - val_loss: 0.4902 - val_categorical_accuracy: 0.8000\n",
      "\n",
      "Epoch 00027: saving model to model_init_2020-12-2104_40_49.443651/model-00027-0.44221-0.83582-0.49023-0.80000.h5\n",
      "\n",
      "Epoch 00027: ReduceLROnPlateau reducing learning rate to 1.2207031829802872e-07.\n",
      "Epoch 28/30\n",
      "67/67 [==============================] - 35s 527ms/step - loss: 0.4069 - categorical_accuracy: 0.8570 - val_loss: 0.5715 - val_categorical_accuracy: 0.8350\n",
      "\n",
      "Epoch 00028: saving model to model_init_2020-12-2104_40_49.443651/model-00028-0.40691-0.85697-0.57151-0.83500.h5\n",
      "Epoch 29/30\n",
      "67/67 [==============================] - 34s 511ms/step - loss: 0.4360 - categorical_accuracy: 0.8246 - val_loss: 0.6829 - val_categorical_accuracy: 0.7750\n",
      "\n",
      "Epoch 00029: saving model to model_init_2020-12-2104_40_49.443651/model-00029-0.43603-0.82463-0.68292-0.77500.h5\n",
      "\n",
      "Epoch 00029: ReduceLROnPlateau reducing learning rate to 6.103515914901436e-08.\n",
      "Epoch 30/30\n",
      "67/67 [==============================] - 35s 517ms/step - loss: 0.4110 - categorical_accuracy: 0.8470 - val_loss: 0.3821 - val_categorical_accuracy: 0.8500\n",
      "\n",
      "Epoch 00030: saving model to model_init_2020-12-2104_40_49.443651/model-00030-0.41097-0.84701-0.38209-0.85000.h5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fe174506668>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 5\n",
    "train_generator = aug_generator(train_path, train_doc, batch_size)\n",
    "val_generator = aug_generator(val_path, val_doc, batch_size)\n",
    "num_epochs = 30\n",
    "model1a.fit_generator(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1, \n",
    "                    callbacks=callbacks_list, validation_data=val_generator, \n",
    "                    validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### It has validation accuracy of 87%. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### New generator with change in data augmentation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aug_generator(source_path, folder_list, batch_size):\n",
    "      \n",
    "    print( 'Source path = ', source_path, '; batch size =', batch_size)\n",
    "    img_idx = [x for x in range(0,nb_frames)] #create a list of image numbers you want to use for a particular video\n",
    "    while True:\n",
    "        t = np.random.permutation(folder_list)\n",
    "        num_batches = len(folder_list)//batch_size # calculate the number of batches\n",
    "        for batch in range(num_batches): # we iterate over the number of batches\n",
    "            batch_data = np.zeros((batch_size,nb_frames,nb_rows,nb_cols,nb_channel)) # x is the number of images you use for each video, (y,z) is the final size of the input images and 3 is the number of channels RGB\n",
    "            batch_labels = np.zeros((batch_size,5)) # batch_labels is the one hot representation of the output\n",
    "            \n",
    "            batch_data_aug = np.zeros((batch_size,nb_frames,nb_rows,nb_cols,nb_channel)) # x is the number of images you use for each video, (y,z) is the final size of the input images and 3 is the number of channels RGB\n",
    "            batch_labels_aug = np.zeros((batch_size,5)) # batch_labels is the one hot representation of the output\n",
    "  \n",
    "            for folder in range(batch_size): # iterate over the batch_size\n",
    "                imgs = os.listdir(source_path+'/'+ t[folder + (batch*batch_size)].split(';')[0]) # read all the images in the folder\n",
    "                M = get_random_affine()\n",
    "                for idx,item in enumerate(img_idx): #  Iterate iver the frames/images of a folder to read them in\n",
    "                    image = cv2.imread(source_path+'/'+ t[folder + (batch*batch_size)].strip().split(';')[0]+'/'+imgs[item], cv2.IMREAD_COLOR)\n",
    "                    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "                    #crop the images and resize them. Note that the images are of 2 different shape \n",
    "                    #and the conv3D will throw error if the inputs in a batch have different shapes   \n",
    "                    resized = cv2.resize(image, (nb_rows,nb_cols), interpolation = cv2.INTER_AREA)\n",
    "                    batch_data[folder,idx] = resized\n",
    "                    batch_data_aug[folder,idx] = cv2.warpAffine(resized, M, (resized.shape[0], resized.shape[1]))\n",
    "                    \n",
    "                batch_labels[folder, int(t[folder + (batch*batch_size)].strip().split(';')[2])] = 1\n",
    "                batch_labels_aug[folder, int(t[folder + (batch*batch_size)].strip().split(';')[2])] = 1\n",
    "            \n",
    "            batch_data = np.append(batch_data, batch_data_aug, axis = 0) \n",
    "            batch_labels = np.append(batch_labels, batch_labels_aug, axis = 0) \n",
    "            yield batch_data, batch_labels #you yield the batch_data and the batch_labels, remember what does yield do\n",
    "\n",
    "        \n",
    "        # write the code for the remaining data points which are left after full batches\n",
    "        if (len(folder_list) != batch_size*num_batches):\n",
    "            print(\"Batch: \",num_batches+1,\"Index:\", batch_size)\n",
    "            batch_size = len(folder_list) - (batch_size*num_batches)\n",
    "            \n",
    "            batch_data = np.zeros((batch_size,nb_frames,nb_rows,nb_cols,nb_channel)) # x is the number of images you use for each video, (y,z) is the final size of the input images and 3 is the number of channels RGB\n",
    "            batch_labels = np.zeros((batch_size,5)) # batch_labels is the one hot representation of the output\n",
    "            \n",
    "            batch_data_aug = np.zeros((batch_size,nb_frames,nb_rows,nb_cols,nb_channel)) # x is the number of images you use for each video, (y,z) is the final size of the input images and 3 is the number of channels RGB\n",
    "            batch_labels_aug = np.zeros((batch_size,5)) # batch_labels is the one hot representation of the output\n",
    "\n",
    "            for folder in range(batch_size): # iterate over the batch_size\n",
    "                imgs = os.listdir(source_path+'/'+ t[folder + (batch*batch_size)].split(';')[0]) # read all the images in the folder\n",
    "                M = get_random_affine()\n",
    "                for idx,item in enumerate(img_idx): #  Iterate iver the frames/images of a folder to read them in\n",
    "                    image = cv2.imread(source_path+'/'+ t[folder + (batch*batch_size)].strip().split(';')[0]+'/'+imgs[item], cv2.IMREAD_COLOR)\n",
    "                    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "                    #crop the images and resize them. Note that the images are of 2 different shape \n",
    "                    #and the conv3D will throw error if the inputs in a batch have different shapes\n",
    "                    resized = cv2.resize(image, (nb_rows,nb_cols), interpolation = cv2.INTER_AREA)\n",
    "                    batch_data[folder,idx] = resized\n",
    "                    batch_data_aug[folder,idx] = cv2.warpAffine(resized, M, (resized.shape[0], resized.shape[1]))\n",
    "                    \n",
    "                batch_labels[folder, int(t[folder + (batch*batch_size)].strip().split(';')[2])] = 1\n",
    "                batch_labels_aug[folder, int(t[folder + (batch*batch_size)].strip().split(';')[2])] = 1\n",
    "            \n",
    "            batch_data = np.append(batch_data, batch_data_aug, axis = 0) \n",
    "            batch_labels = np.append(batch_labels, batch_labels_aug, axis = 0) \n",
    "\n",
    "            yield batch_data, batch_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model 9: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv3d_21 (Conv3D)           (None, 30, 120, 120, 8)   656       \n",
      "_________________________________________________________________\n",
      "batch_normalization_21 (Batc (None, 30, 120, 120, 8)   32        \n",
      "_________________________________________________________________\n",
      "activation_21 (Activation)   (None, 30, 120, 120, 8)   0         \n",
      "_________________________________________________________________\n",
      "max_pooling3d_21 (MaxPooling (None, 15, 60, 60, 8)     0         \n",
      "_________________________________________________________________\n",
      "conv3d_22 (Conv3D)           (None, 15, 60, 60, 16)    3472      \n",
      "_________________________________________________________________\n",
      "batch_normalization_22 (Batc (None, 15, 60, 60, 16)    64        \n",
      "_________________________________________________________________\n",
      "activation_22 (Activation)   (None, 15, 60, 60, 16)    0         \n",
      "_________________________________________________________________\n",
      "max_pooling3d_22 (MaxPooling (None, 7, 30, 30, 16)     0         \n",
      "_________________________________________________________________\n",
      "conv3d_23 (Conv3D)           (None, 7, 30, 30, 32)     4640      \n",
      "_________________________________________________________________\n",
      "batch_normalization_23 (Batc (None, 7, 30, 30, 32)     128       \n",
      "_________________________________________________________________\n",
      "activation_23 (Activation)   (None, 7, 30, 30, 32)     0         \n",
      "_________________________________________________________________\n",
      "max_pooling3d_23 (MaxPooling (None, 3, 15, 15, 32)     0         \n",
      "_________________________________________________________________\n",
      "conv3d_24 (Conv3D)           (None, 3, 15, 15, 64)     18496     \n",
      "_________________________________________________________________\n",
      "batch_normalization_24 (Batc (None, 3, 15, 15, 64)     256       \n",
      "_________________________________________________________________\n",
      "activation_24 (Activation)   (None, 3, 15, 15, 64)     0         \n",
      "_________________________________________________________________\n",
      "max_pooling3d_24 (MaxPooling (None, 1, 7, 7, 64)       0         \n",
      "_________________________________________________________________\n",
      "flatten_6 (Flatten)          (None, 3136)              0         \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 1000)              3137000   \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 1000)              0         \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 500)               500500    \n",
      "_________________________________________________________________\n",
      "dropout_12 (Dropout)         (None, 500)               0         \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 5)                 2505      \n",
      "=================================================================\n",
      "Total params: 3,667,749\n",
      "Trainable params: 3,667,509\n",
      "Non-trainable params: 240\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "nb_frames = 30 # number of frames\n",
    "nb_rows = 120 # image width\n",
    "nb_cols = 120 # image height \n",
    "\n",
    "nb_classes = 5\n",
    "nb_channel = 3\n",
    "\n",
    "input_shape=(nb_frames,nb_rows,nb_cols,nb_channel)\n",
    "\n",
    "# Define model\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Conv3D(nb_filters[0], \n",
    "                 kernel_size=(3,3,3), \n",
    "                 input_shape=input_shape,\n",
    "                 padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(MaxPooling3D(pool_size=(2,2,2)))\n",
    "\n",
    "model.add(Conv3D(nb_filters[1], \n",
    "                 kernel_size=(3,3,3), \n",
    "                 padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(MaxPooling3D(pool_size=(2,2,2)))\n",
    "\n",
    "model.add(Conv3D(nb_filters[2], \n",
    "                 kernel_size=(1,3,3), \n",
    "                 padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(MaxPooling3D(pool_size=(2,2,2)))\n",
    "\n",
    "model.add(Conv3D(nb_filters[3], \n",
    "                 kernel_size=(1,3,3), \n",
    "                 padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(MaxPooling3D(pool_size=(2,2,2)))\n",
    "\n",
    "#Flatten Layers\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(nb_dense[0], activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(nb_dense[1], activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "#softmax layer\n",
    "model.add(Dense(nb_dense[2], activation='softmax'))\n",
    "model.compile(optimizer=Adam(), loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source path = Source path =  ./Project_data/train ; batch size = 10\n",
      " Epoch 1/20\n",
      "./Project_data/val ; batch size = 10\n",
      "65/67 [============================>.] - ETA: 1s - loss: 3.6024 - categorical_accuracy: 0.3308Batch:  67 Index: 10\n",
      "67/67 [==============================] - 70s 1s/step - loss: 3.5457 - categorical_accuracy: 0.3336 - val_loss: 3.3236 - val_categorical_accuracy: 0.3100\n",
      "\n",
      "Epoch 00001: saving model to model_init_2020-12-2104_40_49.443651/model-00001-3.56637-0.33183-3.32358-0.31000.h5\n",
      "Epoch 2/20\n",
      "67/67 [==============================] - 20s 299ms/step - loss: 1.9838 - categorical_accuracy: 0.3159 - val_loss: 1.4836 - val_categorical_accuracy: 0.4200\n",
      "\n",
      "Epoch 00002: saving model to model_init_2020-12-2104_40_49.443651/model-00002-1.98381-0.31592-1.48357-0.42000.h5\n",
      "Epoch 3/20\n",
      "67/67 [==============================] - 22s 326ms/step - loss: 1.5808 - categorical_accuracy: 0.4104 - val_loss: 1.1495 - val_categorical_accuracy: 0.5150\n",
      "\n",
      "Epoch 00003: saving model to model_init_2020-12-2104_40_49.443651/model-00003-1.58078-0.41045-1.14946-0.51500.h5\n",
      "Epoch 4/20\n",
      "67/67 [==============================] - 21s 320ms/step - loss: 1.4474 - categorical_accuracy: 0.4080 - val_loss: 1.2068 - val_categorical_accuracy: 0.5650\n",
      "\n",
      "Epoch 00004: saving model to model_init_2020-12-2104_40_49.443651/model-00004-1.44736-0.40796-1.20676-0.56500.h5\n",
      "Epoch 5/20\n",
      "67/67 [==============================] - 20s 294ms/step - loss: 1.3449 - categorical_accuracy: 0.4701 - val_loss: 1.7554 - val_categorical_accuracy: 0.2850\n",
      "\n",
      "Epoch 00005: saving model to model_init_2020-12-2104_40_49.443651/model-00005-1.34492-0.47015-1.75541-0.28500.h5\n",
      "\n",
      "Epoch 00005: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "Epoch 6/20\n",
      "67/67 [==============================] - 21s 316ms/step - loss: 1.1734 - categorical_accuracy: 0.4925 - val_loss: 1.0128 - val_categorical_accuracy: 0.6250\n",
      "\n",
      "Epoch 00006: saving model to model_init_2020-12-2104_40_49.443651/model-00006-1.17341-0.49254-1.01281-0.62500.h5\n",
      "Epoch 7/20\n",
      "67/67 [==============================] - 20s 305ms/step - loss: 1.2052 - categorical_accuracy: 0.5050 - val_loss: 0.9776 - val_categorical_accuracy: 0.6400\n",
      "\n",
      "Epoch 00007: saving model to model_init_2020-12-2104_40_49.443651/model-00007-1.20519-0.50498-0.97761-0.64000.h5\n",
      "Epoch 8/20\n",
      "67/67 [==============================] - 20s 301ms/step - loss: 1.0336 - categorical_accuracy: 0.5796 - val_loss: 0.9378 - val_categorical_accuracy: 0.6650\n",
      "\n",
      "Epoch 00008: saving model to model_init_2020-12-2104_40_49.443651/model-00008-1.03358-0.57960-0.93783-0.66500.h5\n",
      "Epoch 9/20\n",
      "67/67 [==============================] - 20s 301ms/step - loss: 0.9179 - categorical_accuracy: 0.6269 - val_loss: 0.7991 - val_categorical_accuracy: 0.7000\n",
      "\n",
      "Epoch 00009: saving model to model_init_2020-12-2104_40_49.443651/model-00009-0.91789-0.62687-0.79914-0.70000.h5\n",
      "Epoch 10/20\n",
      "67/67 [==============================] - 20s 304ms/step - loss: 1.1111 - categorical_accuracy: 0.5647 - val_loss: 0.8770 - val_categorical_accuracy: 0.6850\n",
      "\n",
      "Epoch 00010: saving model to model_init_2020-12-2104_40_49.443651/model-00010-1.11107-0.56468-0.87701-0.68500.h5\n",
      "Epoch 11/20\n",
      "67/67 [==============================] - 20s 295ms/step - loss: 0.9863 - categorical_accuracy: 0.6393 - val_loss: 0.9124 - val_categorical_accuracy: 0.6900\n",
      "\n",
      "Epoch 00011: saving model to model_init_2020-12-2104_40_49.443651/model-00011-0.98634-0.63930-0.91237-0.69000.h5\n",
      "\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "Epoch 12/20\n",
      "67/67 [==============================] - 21s 312ms/step - loss: 0.8629 - categorical_accuracy: 0.6866 - val_loss: 0.7055 - val_categorical_accuracy: 0.7500\n",
      "\n",
      "Epoch 00012: saving model to model_init_2020-12-2104_40_49.443651/model-00012-0.86294-0.68657-0.70551-0.75000.h5\n",
      "Epoch 13/20\n",
      "67/67 [==============================] - 20s 305ms/step - loss: 0.7734 - categorical_accuracy: 0.7015 - val_loss: 0.7446 - val_categorical_accuracy: 0.6800\n",
      "\n",
      "Epoch 00013: saving model to model_init_2020-12-2104_40_49.443651/model-00013-0.77345-0.70149-0.74461-0.68000.h5\n",
      "Epoch 14/20\n",
      "67/67 [==============================] - 20s 303ms/step - loss: 0.7546 - categorical_accuracy: 0.7289 - val_loss: 0.6294 - val_categorical_accuracy: 0.7900\n",
      "\n",
      "Epoch 00014: saving model to model_init_2020-12-2104_40_49.443651/model-00014-0.75455-0.72886-0.62937-0.79000.h5\n",
      "Epoch 15/20\n",
      "67/67 [==============================] - 21s 306ms/step - loss: 0.7578 - categorical_accuracy: 0.7214 - val_loss: 0.7356 - val_categorical_accuracy: 0.7100\n",
      "\n",
      "Epoch 00015: saving model to model_init_2020-12-2104_40_49.443651/model-00015-0.75780-0.72139-0.73559-0.71000.h5\n",
      "Epoch 16/20\n",
      "67/67 [==============================] - 20s 299ms/step - loss: 0.8356 - categorical_accuracy: 0.6592 - val_loss: 0.6410 - val_categorical_accuracy: 0.7650\n",
      "\n",
      "Epoch 00016: saving model to model_init_2020-12-2104_40_49.443651/model-00016-0.83560-0.65920-0.64101-0.76500.h5\n",
      "\n",
      "Epoch 00016: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "Epoch 17/20\n",
      "67/67 [==============================] - 20s 300ms/step - loss: 0.6696 - categorical_accuracy: 0.7388 - val_loss: 0.6220 - val_categorical_accuracy: 0.7900\n",
      "\n",
      "Epoch 00017: saving model to model_init_2020-12-2104_40_49.443651/model-00017-0.66962-0.73881-0.62201-0.79000.h5\n",
      "Epoch 18/20\n",
      "67/67 [==============================] - 20s 301ms/step - loss: 0.7223 - categorical_accuracy: 0.7264 - val_loss: 0.6306 - val_categorical_accuracy: 0.7500\n",
      "\n",
      "Epoch 00018: saving model to model_init_2020-12-2104_40_49.443651/model-00018-0.72228-0.72637-0.63064-0.75000.h5\n",
      "Epoch 19/20\n",
      "67/67 [==============================] - 21s 312ms/step - loss: 0.5712 - categorical_accuracy: 0.7637 - val_loss: 0.5342 - val_categorical_accuracy: 0.8250\n",
      "\n",
      "Epoch 00019: saving model to model_init_2020-12-2104_40_49.443651/model-00019-0.57118-0.76368-0.53419-0.82500.h5\n",
      "Epoch 20/20\n",
      "67/67 [==============================] - 20s 299ms/step - loss: 0.6756 - categorical_accuracy: 0.7289 - val_loss: 0.5889 - val_categorical_accuracy: 0.7900\n",
      "\n",
      "Epoch 00020: saving model to model_init_2020-12-2104_40_49.443651/model-00020-0.67564-0.72886-0.58886-0.79000.h5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fe1b034b7b8>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 10\n",
    "train_generator = aug_generator(train_path, train_doc, batch_size)\n",
    "val_generator = aug_generator(val_path, val_doc, batch_size)\n",
    "num_epochs = 20\n",
    "model.fit_generator(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1, \n",
    "                    callbacks=callbacks_list, validation_data=val_generator, \n",
    "                    validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The accuracy is 82.5%\n",
    "\n",
    "##### Changing the Batch Size to 5 and Epoch to 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source path =  ./Project_data/val ; batch size = 5\n",
      "Source path =  ./Project_data/train ; batch size = 5\n",
      "Epoch 1/20\n",
      "67/67 [==============================] - 35s 521ms/step - loss: 0.5239 - categorical_accuracy: 0.8119 - val_loss: 0.5179 - val_categorical_accuracy: 0.7900\n",
      "\n",
      "Epoch 00001: saving model to model_init_2020-12-2104_40_49.443651/model-00001-0.52389-0.81194-0.51787-0.79000.h5\n",
      "Epoch 2/20\n",
      "64/67 [===========================>..] - ETA: 1s - loss: 0.5446 - categorical_accuracy: 0.8094Batch:  133 Index: 5\n",
      "67/67 [==============================] - 33s 490ms/step - loss: 0.5445 - categorical_accuracy: 0.8090 - val_loss: 0.5725 - val_categorical_accuracy: 0.7400\n",
      "\n",
      "Epoch 00002: saving model to model_init_2020-12-2104_40_49.443651/model-00002-0.54368-0.80967-0.57249-0.74000.h5\n",
      "Epoch 3/20\n",
      "67/67 [==============================] - 20s 296ms/step - loss: 0.6180 - categorical_accuracy: 0.7587 - val_loss: 0.4990 - val_categorical_accuracy: 0.8900\n",
      "\n",
      "Epoch 00003: saving model to model_init_2020-12-2104_40_49.443651/model-00003-0.61803-0.75871-0.49895-0.89000.h5\n",
      "Epoch 4/20\n",
      "67/67 [==============================] - 19s 280ms/step - loss: 0.4946 - categorical_accuracy: 0.8308 - val_loss: 0.6429 - val_categorical_accuracy: 0.6400\n",
      "\n",
      "Epoch 00004: saving model to model_init_2020-12-2104_40_49.443651/model-00004-0.49464-0.83085-0.64290-0.64000.h5\n",
      "Epoch 5/20\n",
      "67/67 [==============================] - 19s 291ms/step - loss: 0.5648 - categorical_accuracy: 0.7861 - val_loss: 0.6258 - val_categorical_accuracy: 0.7200\n",
      "\n",
      "Epoch 00005: saving model to model_init_2020-12-2104_40_49.443651/model-00005-0.56477-0.78607-0.62576-0.72000.h5\n",
      "\n",
      "Epoch 00005: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "Epoch 6/20\n",
      "67/67 [==============================] - 19s 278ms/step - loss: 0.5135 - categorical_accuracy: 0.8234 - val_loss: 0.4524 - val_categorical_accuracy: 0.8200\n",
      "\n",
      "Epoch 00006: saving model to model_init_2020-12-2104_40_49.443651/model-00006-0.51352-0.82338-0.45236-0.82000.h5\n",
      "Epoch 7/20\n",
      "67/67 [==============================] - 19s 286ms/step - loss: 0.4431 - categorical_accuracy: 0.8333 - val_loss: 0.4988 - val_categorical_accuracy: 0.8400\n",
      "\n",
      "Epoch 00007: saving model to model_init_2020-12-2104_40_49.443651/model-00007-0.44312-0.83333-0.49875-0.84000.h5\n",
      "Epoch 8/20\n",
      "67/67 [==============================] - 20s 294ms/step - loss: 0.5060 - categorical_accuracy: 0.8209 - val_loss: 0.5542 - val_categorical_accuracy: 0.7600\n",
      "\n",
      "Epoch 00008: saving model to model_init_2020-12-2104_40_49.443651/model-00008-0.50599-0.82090-0.55418-0.76000.h5\n",
      "\n",
      "Epoch 00008: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "Epoch 9/20\n",
      "67/67 [==============================] - 19s 283ms/step - loss: 0.4754 - categorical_accuracy: 0.8159 - val_loss: 0.5473 - val_categorical_accuracy: 0.7500\n",
      "\n",
      "Epoch 00009: saving model to model_init_2020-12-2104_40_49.443651/model-00009-0.47540-0.81592-0.54733-0.75000.h5\n",
      "Epoch 10/20\n",
      "67/67 [==============================] - 20s 293ms/step - loss: 0.4244 - categorical_accuracy: 0.8507 - val_loss: 0.5137 - val_categorical_accuracy: 0.7800\n",
      "\n",
      "Epoch 00010: saving model to model_init_2020-12-2104_40_49.443651/model-00010-0.42439-0.85075-0.51367-0.78000.h5\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
      "Epoch 11/20\n",
      "67/67 [==============================] - 19s 282ms/step - loss: 0.4873 - categorical_accuracy: 0.8284 - val_loss: 0.6575 - val_categorical_accuracy: 0.7100\n",
      "\n",
      "Epoch 00011: saving model to model_init_2020-12-2104_40_49.443651/model-00011-0.48733-0.82836-0.65747-0.71000.h5\n",
      "Epoch 12/20\n",
      "67/67 [==============================] - 18s 269ms/step - loss: 0.4467 - categorical_accuracy: 0.8483 - val_loss: 0.4030 - val_categorical_accuracy: 0.8200\n",
      "\n",
      "Epoch 00012: saving model to model_init_2020-12-2104_40_49.443651/model-00012-0.44672-0.84826-0.40297-0.82000.h5\n",
      "Epoch 13/20\n",
      "67/67 [==============================] - 19s 279ms/step - loss: 0.4823 - categorical_accuracy: 0.8209 - val_loss: 0.6727 - val_categorical_accuracy: 0.6700\n",
      "\n",
      "Epoch 00013: saving model to model_init_2020-12-2104_40_49.443651/model-00013-0.48228-0.82090-0.67267-0.67000.h5\n",
      "Epoch 14/20\n",
      "67/67 [==============================] - 18s 270ms/step - loss: 0.4726 - categorical_accuracy: 0.8184 - val_loss: 0.3826 - val_categorical_accuracy: 0.8800\n",
      "\n",
      "Epoch 00014: saving model to model_init_2020-12-2104_40_49.443651/model-00014-0.47260-0.81841-0.38255-0.88000.h5\n",
      "Epoch 15/20\n",
      "67/67 [==============================] - 18s 274ms/step - loss: 0.4233 - categorical_accuracy: 0.8458 - val_loss: 0.5763 - val_categorical_accuracy: 0.7300\n",
      "\n",
      "Epoch 00015: saving model to model_init_2020-12-2104_40_49.443651/model-00015-0.42332-0.84577-0.57632-0.73000.h5\n",
      "Epoch 16/20\n",
      "67/67 [==============================] - 18s 265ms/step - loss: 0.3970 - categorical_accuracy: 0.8731 - val_loss: 0.4733 - val_categorical_accuracy: 0.8200\n",
      "\n",
      "Epoch 00016: saving model to model_init_2020-12-2104_40_49.443651/model-00016-0.39705-0.87313-0.47326-0.82000.h5\n",
      "\n",
      "Epoch 00016: ReduceLROnPlateau reducing learning rate to 7.812500371073838e-06.\n",
      "Epoch 17/20\n",
      "67/67 [==============================] - 19s 279ms/step - loss: 0.4223 - categorical_accuracy: 0.8507 - val_loss: 0.5120 - val_categorical_accuracy: 0.7800\n",
      "\n",
      "Epoch 00017: saving model to model_init_2020-12-2104_40_49.443651/model-00017-0.42229-0.85075-0.51204-0.78000.h5\n",
      "Epoch 18/20\n",
      "67/67 [==============================] - 19s 280ms/step - loss: 0.4825 - categorical_accuracy: 0.8184 - val_loss: 0.5380 - val_categorical_accuracy: 0.7600\n",
      "\n",
      "Epoch 00018: saving model to model_init_2020-12-2104_40_49.443651/model-00018-0.48251-0.81841-0.53799-0.76000.h5\n",
      "\n",
      "Epoch 00018: ReduceLROnPlateau reducing learning rate to 3.906250185536919e-06.\n",
      "Epoch 19/20\n",
      "67/67 [==============================] - 18s 273ms/step - loss: 0.4182 - categorical_accuracy: 0.8483 - val_loss: 0.4943 - val_categorical_accuracy: 0.7800\n",
      "\n",
      "Epoch 00019: saving model to model_init_2020-12-2104_40_49.443651/model-00019-0.41818-0.84826-0.49435-0.78000.h5\n",
      "Epoch 20/20\n",
      "67/67 [==============================] - 19s 284ms/step - loss: 0.5013 - categorical_accuracy: 0.8134 - val_loss: 0.5424 - val_categorical_accuracy: 0.7600\n",
      "\n",
      "Epoch 00020: saving model to model_init_2020-12-2104_40_49.443651/model-00020-0.50133-0.81343-0.54243-0.76000.h5\n",
      "\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 1.9531250927684596e-06.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fe1882ae978>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 5\n",
    "train_generator = aug_generator(train_path, train_doc, batch_size)\n",
    "val_generator = aug_generator(val_path, val_doc, batch_size)\n",
    "num_epochs = 20\n",
    "model.fit_generator(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1, \n",
    "                    callbacks=callbacks_list, validation_data=val_generator, \n",
    "                    validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### It has a vaidation accuracy of 89%. \n",
    "\n",
    "##### Changing the batch size and epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source path =  ./Project_data/val ; batch size = 20\n",
      "Source path =  ./Project_data/train ; batch size = 20\n",
      "Epoch 1/20\n",
      "32/67 [=============>................] - ETA: 1:07 - loss: 0.3606 - categorical_accuracy: 0.8813Batch:  34 Index: 20\n",
      "67/67 [==============================] - 78s 1s/step - loss: 0.3725 - categorical_accuracy: 0.8719 - val_loss: 0.5239 - val_categorical_accuracy: 0.7775\n",
      "\n",
      "Epoch 00001: saving model to model_init_2020-12-2104_40_49.443651/model-00001-0.36329-0.87861-0.52386-0.77750.h5\n",
      "Epoch 2/20\n",
      "67/67 [==============================] - 23s 350ms/step - loss: 0.4654 - categorical_accuracy: 0.8657 - val_loss: 0.5315 - val_categorical_accuracy: 0.7750\n",
      "\n",
      "Epoch 00002: saving model to model_init_2020-12-2104_40_49.443651/model-00002-0.46543-0.86567-0.53146-0.77500.h5\n",
      "Epoch 3/20\n",
      "67/67 [==============================] - 25s 370ms/step - loss: 0.4629 - categorical_accuracy: 0.8234 - val_loss: 0.5250 - val_categorical_accuracy: 0.7825\n",
      "\n",
      "Epoch 00003: saving model to model_init_2020-12-2104_40_49.443651/model-00003-0.46286-0.82338-0.52495-0.78250.h5\n",
      "\n",
      "Epoch 00003: ReduceLROnPlateau reducing learning rate to 9.765625463842298e-07.\n",
      "Epoch 4/20\n",
      "67/67 [==============================] - 25s 371ms/step - loss: 0.3987 - categorical_accuracy: 0.8756 - val_loss: 0.5279 - val_categorical_accuracy: 0.7725\n",
      "\n",
      "Epoch 00004: saving model to model_init_2020-12-2104_40_49.443651/model-00004-0.39875-0.87562-0.52789-0.77250.h5\n",
      "Epoch 5/20\n",
      "67/67 [==============================] - 24s 352ms/step - loss: 0.4042 - categorical_accuracy: 0.8582 - val_loss: 0.5189 - val_categorical_accuracy: 0.7825\n",
      "\n",
      "Epoch 00005: saving model to model_init_2020-12-2104_40_49.443651/model-00005-0.40417-0.85821-0.51892-0.78250.h5\n",
      "Epoch 6/20\n",
      "67/67 [==============================] - 25s 370ms/step - loss: 0.4488 - categorical_accuracy: 0.8408 - val_loss: 0.5287 - val_categorical_accuracy: 0.7675\n",
      "\n",
      "Epoch 00006: saving model to model_init_2020-12-2104_40_49.443651/model-00006-0.44879-0.84080-0.52866-0.76750.h5\n",
      "Epoch 7/20\n",
      "67/67 [==============================] - 24s 354ms/step - loss: 0.4148 - categorical_accuracy: 0.8657 - val_loss: 0.5298 - val_categorical_accuracy: 0.7650\n",
      "\n",
      "Epoch 00007: saving model to model_init_2020-12-2104_40_49.443651/model-00007-0.41483-0.86567-0.52984-0.76500.h5\n",
      "\n",
      "Epoch 00007: ReduceLROnPlateau reducing learning rate to 4.882812731921149e-07.\n",
      "Epoch 8/20\n",
      "67/67 [==============================] - 25s 367ms/step - loss: 0.4058 - categorical_accuracy: 0.8507 - val_loss: 0.5235 - val_categorical_accuracy: 0.7800\n",
      "\n",
      "Epoch 00008: saving model to model_init_2020-12-2104_40_49.443651/model-00008-0.40584-0.85075-0.52350-0.78000.h5\n",
      "Epoch 9/20\n",
      "67/67 [==============================] - 23s 348ms/step - loss: 0.3999 - categorical_accuracy: 0.8582 - val_loss: 0.5208 - val_categorical_accuracy: 0.7750\n",
      "\n",
      "Epoch 00009: saving model to model_init_2020-12-2104_40_49.443651/model-00009-0.39990-0.85821-0.52083-0.77500.h5\n",
      "\n",
      "Epoch 00009: ReduceLROnPlateau reducing learning rate to 2.4414063659605745e-07.\n",
      "Epoch 10/20\n",
      "67/67 [==============================] - 25s 370ms/step - loss: 0.4650 - categorical_accuracy: 0.8433 - val_loss: 0.5283 - val_categorical_accuracy: 0.7775\n",
      "\n",
      "Epoch 00010: saving model to model_init_2020-12-2104_40_49.443651/model-00010-0.46500-0.84328-0.52833-0.77750.h5\n",
      "Epoch 11/20\n",
      "67/67 [==============================] - 24s 351ms/step - loss: 0.3464 - categorical_accuracy: 0.8756 - val_loss: 0.5281 - val_categorical_accuracy: 0.7800\n",
      "\n",
      "Epoch 00011: saving model to model_init_2020-12-2104_40_49.443651/model-00011-0.34641-0.87562-0.52807-0.78000.h5\n",
      "\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 1.2207031829802872e-07.\n",
      "Epoch 12/20\n",
      "67/67 [==============================] - 24s 356ms/step - loss: 0.4068 - categorical_accuracy: 0.8532 - val_loss: 0.5262 - val_categorical_accuracy: 0.7800\n",
      "\n",
      "Epoch 00012: saving model to model_init_2020-12-2104_40_49.443651/model-00012-0.40676-0.85323-0.52616-0.78000.h5\n",
      "Epoch 13/20\n",
      "67/67 [==============================] - 24s 355ms/step - loss: 0.4507 - categorical_accuracy: 0.8308 - val_loss: 0.5263 - val_categorical_accuracy: 0.7750\n",
      "\n",
      "Epoch 00013: saving model to model_init_2020-12-2104_40_49.443651/model-00013-0.45069-0.83085-0.52630-0.77500.h5\n",
      "\n",
      "Epoch 00013: ReduceLROnPlateau reducing learning rate to 6.103515914901436e-08.\n",
      "Epoch 14/20\n",
      "67/67 [==============================] - 24s 359ms/step - loss: 0.4312 - categorical_accuracy: 0.8333 - val_loss: 0.5253 - val_categorical_accuracy: 0.7750\n",
      "\n",
      "Epoch 00014: saving model to model_init_2020-12-2104_40_49.443651/model-00014-0.43119-0.83333-0.52532-0.77500.h5\n",
      "Epoch 15/20\n",
      "67/67 [==============================] - 24s 351ms/step - loss: 0.3956 - categorical_accuracy: 0.8607 - val_loss: 0.5232 - val_categorical_accuracy: 0.7775\n",
      "\n",
      "Epoch 00015: saving model to model_init_2020-12-2104_40_49.443651/model-00015-0.39559-0.86070-0.52316-0.77750.h5\n",
      "\n",
      "Epoch 00015: ReduceLROnPlateau reducing learning rate to 3.051757957450718e-08.\n",
      "Epoch 16/20\n",
      "67/67 [==============================] - 24s 351ms/step - loss: 0.4513 - categorical_accuracy: 0.8308 - val_loss: 0.5283 - val_categorical_accuracy: 0.7775\n",
      "\n",
      "Epoch 00016: saving model to model_init_2020-12-2104_40_49.443651/model-00016-0.45131-0.83085-0.52830-0.77750.h5\n",
      "Epoch 17/20\n",
      "67/67 [==============================] - 23s 347ms/step - loss: 0.4381 - categorical_accuracy: 0.8333 - val_loss: 0.5243 - val_categorical_accuracy: 0.7775\n",
      "\n",
      "Epoch 00017: saving model to model_init_2020-12-2104_40_49.443651/model-00017-0.43807-0.83333-0.52428-0.77750.h5\n",
      "\n",
      "Epoch 00017: ReduceLROnPlateau reducing learning rate to 1.525878978725359e-08.\n",
      "Epoch 18/20\n",
      "67/67 [==============================] - 24s 359ms/step - loss: 0.5584 - categorical_accuracy: 0.7935 - val_loss: 0.5247 - val_categorical_accuracy: 0.7725\n",
      "\n",
      "Epoch 00018: saving model to model_init_2020-12-2104_40_49.443651/model-00018-0.55840-0.79353-0.52469-0.77250.h5\n",
      "Epoch 19/20\n",
      "67/67 [==============================] - 23s 347ms/step - loss: 0.3874 - categorical_accuracy: 0.8632 - val_loss: 0.5248 - val_categorical_accuracy: 0.7725\n",
      "\n",
      "Epoch 00019: saving model to model_init_2020-12-2104_40_49.443651/model-00019-0.38736-0.86318-0.52475-0.77250.h5\n",
      "\n",
      "Epoch 00019: ReduceLROnPlateau reducing learning rate to 7.629394893626795e-09.\n",
      "Epoch 20/20\n",
      "67/67 [==============================] - 24s 353ms/step - loss: 0.3598 - categorical_accuracy: 0.8582 - val_loss: 0.5228 - val_categorical_accuracy: 0.7825\n",
      "\n",
      "Epoch 00020: saving model to model_init_2020-12-2104_40_49.443651/model-00020-0.35979-0.85821-0.52282-0.78250.h5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fe1882ae7f0>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 20\n",
    "train_generator = aug_generator(train_path, train_doc, batch_size)\n",
    "val_generator = aug_generator(val_path, val_doc, batch_size)\n",
    "num_epochs = 20\n",
    "model.fit_generator(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1, \n",
    "                    callbacks=callbacks_list, validation_data=val_generator, \n",
    "                    validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### It has a ccuracy of 78.35%. \n",
    "\n",
    "### Model 10: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv3d_25 (Conv3D)           (None, 30, 120, 120, 8)   656       \n",
      "_________________________________________________________________\n",
      "batch_normalization_25 (Batc (None, 30, 120, 120, 8)   32        \n",
      "_________________________________________________________________\n",
      "activation_25 (Activation)   (None, 30, 120, 120, 8)   0         \n",
      "_________________________________________________________________\n",
      "max_pooling3d_25 (MaxPooling (None, 15, 60, 60, 8)     0         \n",
      "_________________________________________________________________\n",
      "conv3d_26 (Conv3D)           (None, 15, 60, 60, 16)    3472      \n",
      "_________________________________________________________________\n",
      "batch_normalization_26 (Batc (None, 15, 60, 60, 16)    64        \n",
      "_________________________________________________________________\n",
      "activation_26 (Activation)   (None, 15, 60, 60, 16)    0         \n",
      "_________________________________________________________________\n",
      "max_pooling3d_26 (MaxPooling (None, 7, 30, 30, 16)     0         \n",
      "_________________________________________________________________\n",
      "conv3d_27 (Conv3D)           (None, 7, 30, 30, 32)     4640      \n",
      "_________________________________________________________________\n",
      "batch_normalization_27 (Batc (None, 7, 30, 30, 32)     128       \n",
      "_________________________________________________________________\n",
      "activation_27 (Activation)   (None, 7, 30, 30, 32)     0         \n",
      "_________________________________________________________________\n",
      "max_pooling3d_27 (MaxPooling (None, 3, 15, 15, 32)     0         \n",
      "_________________________________________________________________\n",
      "conv3d_28 (Conv3D)           (None, 3, 15, 15, 64)     18496     \n",
      "_________________________________________________________________\n",
      "activation_28 (Activation)   (None, 3, 15, 15, 64)     0         \n",
      "_________________________________________________________________\n",
      "dropout_13 (Dropout)         (None, 3, 15, 15, 64)     0         \n",
      "_________________________________________________________________\n",
      "max_pooling3d_28 (MaxPooling (None, 1, 7, 7, 64)       0         \n",
      "_________________________________________________________________\n",
      "flatten_7 (Flatten)          (None, 3136)              0         \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 1000)              3137000   \n",
      "_________________________________________________________________\n",
      "dropout_14 (Dropout)         (None, 1000)              0         \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             (None, 500)               500500    \n",
      "_________________________________________________________________\n",
      "dropout_15 (Dropout)         (None, 500)               0         \n",
      "_________________________________________________________________\n",
      "dense_21 (Dense)             (None, 5)                 2505      \n",
      "=================================================================\n",
      "Total params: 3,667,493\n",
      "Trainable params: 3,667,381\n",
      "Non-trainable params: 112\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "nb_frames = 30 # number of frames\n",
    "nb_rows = 120 # image width\n",
    "nb_cols = 120 # image height \n",
    "\n",
    "nb_classes = 5\n",
    "nb_channel = 3\n",
    "\n",
    "input_shape=(nb_frames,nb_rows,nb_cols,nb_channel)\n",
    "\n",
    "# Define model\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Conv3D(nb_filters[0], \n",
    "                 kernel_size=(3,3,3), \n",
    "                 input_shape=input_shape,\n",
    "                 padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(MaxPooling3D(pool_size=(2,2,2)))\n",
    "\n",
    "model.add(Conv3D(nb_filters[1], \n",
    "                 kernel_size=(3,3,3), \n",
    "                 padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(MaxPooling3D(pool_size=(2,2,2)))\n",
    "\n",
    "model.add(Conv3D(nb_filters[2], \n",
    "                 kernel_size=(1,3,3), \n",
    "                 padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(MaxPooling3D(pool_size=(2,2,2)))\n",
    "\n",
    "model.add(Conv3D(nb_filters[3], \n",
    "                 kernel_size=(1,3,3), \n",
    "                 padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(MaxPooling3D(pool_size=(2,2,2)))\n",
    "\n",
    "#Flatten Layers\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(nb_dense[0], activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(nb_dense[1], activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "#softmax layer\n",
    "model.add(Dense(nb_dense[2], activation='softmax'))\n",
    "model.compile(optimizer=Adam(), loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source path = Source path =  ./Project_data/val  ./Project_data/train ; batch size = 10\n",
      "; batch size = 10\n",
      "Epoch 1/20\n",
      "65/67 [============================>.] - ETA: 1s - loss: 1.6934 - categorical_accuracy: 0.3615Batch:  67 Index: 10\n",
      "67/67 [==============================] - 68s 1s/step - loss: 1.6741 - categorical_accuracy: 0.3657 - val_loss: 1.3837 - val_categorical_accuracy: 0.4150\n",
      "\n",
      "Epoch 00001: saving model to model_init_2020-12-2104_40_49.443651/model-00001-1.68101-0.36425-1.38368-0.41500.h5\n",
      "Epoch 2/20\n",
      "67/67 [==============================] - 21s 316ms/step - loss: 1.4462 - categorical_accuracy: 0.3682 - val_loss: 1.7043 - val_categorical_accuracy: 0.3000\n",
      "\n",
      "Epoch 00002: saving model to model_init_2020-12-2104_40_49.443651/model-00002-1.44618-0.36816-1.70431-0.30000.h5\n",
      "Epoch 3/20\n",
      "67/67 [==============================] - 20s 299ms/step - loss: 1.3578 - categorical_accuracy: 0.4552 - val_loss: 1.2815 - val_categorical_accuracy: 0.4500\n",
      "\n",
      "Epoch 00003: saving model to model_init_2020-12-2104_40_49.443651/model-00003-1.35779-0.45522-1.28153-0.45000.h5\n",
      "Epoch 4/20\n",
      "67/67 [==============================] - 21s 307ms/step - loss: 1.3935 - categorical_accuracy: 0.4403 - val_loss: 1.4298 - val_categorical_accuracy: 0.3650\n",
      "\n",
      "Epoch 00004: saving model to model_init_2020-12-2104_40_49.443651/model-00004-1.39351-0.44030-1.42982-0.36500.h5\n",
      "Epoch 5/20\n",
      "67/67 [==============================] - 21s 317ms/step - loss: 1.2781 - categorical_accuracy: 0.4950 - val_loss: 1.3786 - val_categorical_accuracy: 0.3300\n",
      "\n",
      "Epoch 00005: saving model to model_init_2020-12-2104_40_49.443651/model-00005-1.27812-0.49502-1.37862-0.33000.h5\n",
      "\n",
      "Epoch 00005: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "Epoch 6/20\n",
      "67/67 [==============================] - 21s 310ms/step - loss: 1.0979 - categorical_accuracy: 0.5597 - val_loss: 0.8974 - val_categorical_accuracy: 0.6450\n",
      "\n",
      "Epoch 00006: saving model to model_init_2020-12-2104_40_49.443651/model-00006-1.09789-0.55970-0.89745-0.64500.h5\n",
      "Epoch 7/20\n",
      "67/67 [==============================] - 20s 301ms/step - loss: 1.0286 - categorical_accuracy: 0.5572 - val_loss: 0.9051 - val_categorical_accuracy: 0.6550\n",
      "\n",
      "Epoch 00007: saving model to model_init_2020-12-2104_40_49.443651/model-00007-1.02861-0.55721-0.90508-0.65500.h5\n",
      "Epoch 8/20\n",
      "67/67 [==============================] - 21s 308ms/step - loss: 0.9761 - categorical_accuracy: 0.6045 - val_loss: 0.8372 - val_categorical_accuracy: 0.6250\n",
      "\n",
      "Epoch 00008: saving model to model_init_2020-12-2104_40_49.443651/model-00008-0.97610-0.60448-0.83722-0.62500.h5\n",
      "Epoch 9/20\n",
      "67/67 [==============================] - 22s 322ms/step - loss: 0.8369 - categorical_accuracy: 0.6791 - val_loss: 0.7827 - val_categorical_accuracy: 0.6850\n",
      "\n",
      "Epoch 00009: saving model to model_init_2020-12-2104_40_49.443651/model-00009-0.83685-0.67910-0.78270-0.68500.h5\n",
      "Epoch 10/20\n",
      "67/67 [==============================] - 21s 318ms/step - loss: 0.7616 - categorical_accuracy: 0.6990 - val_loss: 0.7547 - val_categorical_accuracy: 0.6700\n",
      "\n",
      "Epoch 00010: saving model to model_init_2020-12-2104_40_49.443651/model-00010-0.76158-0.69900-0.75473-0.67000.h5\n",
      "Epoch 11/20\n",
      "67/67 [==============================] - 20s 299ms/step - loss: 0.7481 - categorical_accuracy: 0.7189 - val_loss: 0.7705 - val_categorical_accuracy: 0.6700\n",
      "\n",
      "Epoch 00011: saving model to model_init_2020-12-2104_40_49.443651/model-00011-0.74807-0.71891-0.77048-0.67000.h5\n",
      "Epoch 12/20\n",
      "67/67 [==============================] - 21s 308ms/step - loss: 0.6483 - categorical_accuracy: 0.7512 - val_loss: 0.6720 - val_categorical_accuracy: 0.7200\n",
      "\n",
      "Epoch 00012: saving model to model_init_2020-12-2104_40_49.443651/model-00012-0.64829-0.75124-0.67198-0.72000.h5\n",
      "Epoch 13/20\n",
      "67/67 [==============================] - 20s 306ms/step - loss: 0.7755 - categorical_accuracy: 0.6990 - val_loss: 0.7469 - val_categorical_accuracy: 0.6700\n",
      "\n",
      "Epoch 00013: saving model to model_init_2020-12-2104_40_49.443651/model-00013-0.77547-0.69900-0.74692-0.67000.h5\n",
      "Epoch 14/20\n",
      "67/67 [==============================] - 20s 305ms/step - loss: 0.6569 - categorical_accuracy: 0.7562 - val_loss: 0.9050 - val_categorical_accuracy: 0.6500\n",
      "\n",
      "Epoch 00014: saving model to model_init_2020-12-2104_40_49.443651/model-00014-0.65687-0.75622-0.90503-0.65000.h5\n",
      "\n",
      "Epoch 00014: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "Epoch 15/20\n",
      "67/67 [==============================] - 20s 303ms/step - loss: 0.4766 - categorical_accuracy: 0.8308 - val_loss: 0.6300 - val_categorical_accuracy: 0.7500\n",
      "\n",
      "Epoch 00015: saving model to model_init_2020-12-2104_40_49.443651/model-00015-0.47662-0.83085-0.63000-0.75000.h5\n",
      "Epoch 16/20\n",
      "67/67 [==============================] - 20s 305ms/step - loss: 0.4955 - categorical_accuracy: 0.8259 - val_loss: 0.6127 - val_categorical_accuracy: 0.7400\n",
      "\n",
      "Epoch 00016: saving model to model_init_2020-12-2104_40_49.443651/model-00016-0.49549-0.82587-0.61270-0.74000.h5\n",
      "Epoch 17/20\n",
      "67/67 [==============================] - 20s 299ms/step - loss: 0.4456 - categorical_accuracy: 0.8035 - val_loss: 0.6831 - val_categorical_accuracy: 0.7200\n",
      "\n",
      "Epoch 00017: saving model to model_init_2020-12-2104_40_49.443651/model-00017-0.44556-0.80348-0.68309-0.72000.h5\n",
      "Epoch 18/20\n",
      "67/67 [==============================] - 20s 298ms/step - loss: 0.4198 - categorical_accuracy: 0.8458 - val_loss: 0.6226 - val_categorical_accuracy: 0.7350\n",
      "\n",
      "Epoch 00018: saving model to model_init_2020-12-2104_40_49.443651/model-00018-0.41983-0.84577-0.62259-0.73500.h5\n",
      "\n",
      "Epoch 00018: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "Epoch 19/20\n",
      "67/67 [==============================] - 20s 296ms/step - loss: 0.3179 - categorical_accuracy: 0.8856 - val_loss: 0.5807 - val_categorical_accuracy: 0.7100\n",
      "\n",
      "Epoch 00019: saving model to model_init_2020-12-2104_40_49.443651/model-00019-0.31794-0.88557-0.58072-0.71000.h5\n",
      "Epoch 20/20\n",
      "67/67 [==============================] - 21s 307ms/step - loss: 0.3501 - categorical_accuracy: 0.8781 - val_loss: 0.6076 - val_categorical_accuracy: 0.7300\n",
      "\n",
      "Epoch 00020: saving model to model_init_2020-12-2104_40_49.443651/model-00020-0.35014-0.87811-0.60758-0.73000.h5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fe18bc33f28>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 10\n",
    "train_generator = aug_generator(train_path, train_doc, batch_size)\n",
    "val_generator = aug_generator(val_path, val_doc, batch_size)\n",
    "num_epochs = 20\n",
    "model.fit_generator(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1, \n",
    "                    callbacks=callbacks_list, validation_data=val_generator, \n",
    "                    validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### It has a accuracy of 74%. \n",
    "\n",
    "### So, the Model 9 with batch size 5 and number of epochs 20 is the best model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
